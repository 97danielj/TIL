# 19-01 잠재 의미 분석(Latent Semantic Analysis, LSA)

LSA는 정확히는 토픽 모델링을 위해 최적화 된 알고리즘은 아니지만, 토픽 모델링이라는 분야에 아이디어를 제공한 알고리즘이라고 볼 수 있습니다. 이에 토픽 모델링 알고리즘인 LDA에 앞서 배워보도록 하겠습니다. 뒤에서 배우게 되는 LDA는 LSA의 단점을 개선하여 탄생한 알고리즘으로 토픽 모델링에 보다 적합한 알고리즘입니다.

BoW에 기반한 DTM이나 TF-IDF는 기본적으로 단어의 빈도 수를 이용한 수치화 방법이기 때문에 단어의 의미를 고려하지 못한다는 단점이 있었습니다. (이를 토픽 모델링 관점에서는 단어의 토픽을 고려하지 못한다고도 합니다.) 이를 위한 대안으로 DTM의 잠재된(Latent) 의미를 이끌어내는 방법으로 잠재 의미 분석(Latent Semantic Analysis, LSA)이라는 방법이 있습니다. 잠재 의미 분석(Latent Semantic Indexing, LSI)이라고 부르기도 합니다. 이하 LSA라고 명명하겠습니다.

이 방법을 이해하기 위해서는 선형대수학의 특이값 분해(Singular Value Decomposition, SVD)를 이해할 필요가 있습니다. 이하 이를 SVD라고 명명하겠습니다. 이 실습에서는 SVD를 수행하는 구체적인 선형대수학에 대해서는 설명하지 않고, SVD가 갖고있는 의미를 이해하는 것에 초점을 맞춥니다.

## 1. 특이값 분해(Singular Value Decomposition, SVD)

시작하기 앞서, 여기서의 특이값 분해(Singular Value Decomposition, SVD)는 실수 벡터 공간에 한정하여 내용을 설명함을 명시합니다. SVD란 A가 m × n 행렬일 때, 다음과 같이 3개의 행렬의 곱으로 분해(decomposition)하는 것을 말합니다.

$A=UΣV^\text{T}$

여기서 각 3개의 행렬은 다음과 같은 조건을 만족합니다.
$U: m × m\ \text{직교행렬}\ (AA^\text{T}=U(ΣΣ^\text{T})U^\text{T})$

$V: n × n\ \text{직교행렬}\ (A^\text{T}A=V(Σ^\text{T}Σ)V^\text{T})$

$Σ: m × n\ \text{직사각 대각행렬}$

여기서 직교행렬(orthogonal matrix)이란 자신과 자신의 전치 행렬(transposed matrix)의 곱 또는 이를 반대로 곱한 결과가 단위행렬(identity matrix)이 되는 행렬을 말합니다. 또한 대각행렬(diagonal matrix)이란 주대각선을 제외한 곳의 원소가 모두 0인 행렬을 의미합니다.

### 1) 전치 행렬(Transposed Matrix)

전치 행렬(transposed matrix)은 원래의 행렬에서 행과 열을 바꾼 행렬입니다. 즉, 주대각선을 축으로 반사 대칭을 하여 얻는 행렬입니다. 기호는 기존 행렬 표현의 우측 위에 $T$를 붙입니다. 예를 들어서 기존의 행렬을 M이라고 한다면, 전치 행렬은 $M^\text{T}$와 같이 표현합니다.
$$
M = 
\left[
    \begin{array}{c}
      1\ 2\\
      3\ 4\\
      5\ 6\\
    \end{array}
  \right]
\ \ \ \
$$

$$
M^\text{T} = 
\left[
    \begin{array}{c}
      1\ 3\ 5\\
      2\ 4\ 6\\
    \end{array}
  \right]
\ \ \ \
$$



### 2) 단위 행렬(Identity Matrix)

단위 행렬(identity matrix)은 주대각선의 원소가 모두 1이며 나머지 원소는 모두 0인 정사각 행렬을 말합니다. 보통 줄여서 대문자 $I$로 표현하기도 하는데, 2 × 2 단위 행렬과 3 × 3 단위 행렬을 표현해보면 다음과 같습니다.
$$
I = 
\left[
    \begin{array}{c}
      1\ 0\\
      0\ 1\\
    \end{array}
  \right]
\ \ \ \
$$

$$
I = 
\left[
    \begin{array}{c}
      1\ 0\ 0\\
      0\ 1\ 0\\
      0\ 0\ 1\\
    \end{array}
  \right]
\ \ \ \
$$

### 3) 역행렬(Inverse Matrix)

단위 행렬(identity matrix)를 이해했다면 역행렬(inverse matrix)을 정의할 수 있습니다. 만약 행렬 A와 어떤 행렬을 곱했을 때, 결과로서 단위 행렬이 나온다면 이때의 어떤 행렬을 A의 역행렬이라고 하며, $A^{-1}$라고 표현합니다.
$$
A\ ×\ A^{-1} = I
$$

$$
\left[
    \begin{array}{c}
      1\ 2\ 3\\
      4\ 5\ 6\\
      7\ 8\ 9\\
    \end{array}
  \right]
×
\left[
    \begin{array}{c}
      \ \ \ \ \ \ \ \ \\
      \ \ \ \ ?\ \ \ \\
      \ \ \ \ \ \ \ \ \\
    \end{array}
  \right]
=
\left[
    \begin{array}{c}
      1\ 0\ 0\\
      0\ 1\ 0\\
      0\ 0\ 1\\
    \end{array}
  \right]
$$



### 4) 직교 행렬(Orthogonal matrix)

다시 직교 행렬(orthogonal matrix)의 정의로 돌아가서, 실수 n×n행렬 A에 대해서 A × $A^T$=$I$를 만족하면서 $A^T\ ×\ A = I$을 만족하는 행렬 A를 직교 행렬이라고 합니다. 그런데 역행렬의 정의를 다시 생각해보면, 결국 직교 행렬은 $A^{-1}=A^{T}$를 만족합니다.

### 5) 대각 행렬(Diagonal matrix)

대각행렬(diagonal matrix)은 주대각선을 제외한 곳의 원소가 모두 0인 행렬을 말합니다. 아래의 그림에서는 주대각선의 원소를 a라고 표현하고 있습니다. 만약 대각 행렬 Σ가 3 × 3 행렬이라면, 다음과 같은 모양을 가집니다.
$$
Σ=
\left[
    \begin{array}{c}
      a\ \ 0\ \ 0\\
      0\ \ a\ \ 0\\
      0\ \ 0\ \ a\\
    \end{array}
  \right]
$$
여기까진 정사각 행렬이기 때문에 직관적으로 이해가 쉽습니다. 그런데 정사각 행렬이 아니라 직사각 행렬이 될 경우를 잘 보아야 헷갈리지 않습니다. 만약 행의 크기가 열의 크기보다 크다면 다음과 같은 모양을 가집니다. 즉, m × n 행렬일 때, m > n인 경우입니다.
$$
Σ=
\left[
    \begin{array}{c}
      a\ \ 0\ \ 0\\
      0\ \ a\ \ 0\\
      0\ \ 0\ \ a\\
      0\ \ 0\ \ 0\\
    \end{array}
  \right]
$$


반면 n > m인 경우에는 다음과 같은 모양을 가집니다.
$$
Σ=
\left[
    \begin{array}{c}
      a\ \ 0\ \ 0\ \ 0\\
      0\ \ a\ \ 0\ \ 0\\
      0\ \ 0\ \ a\ \ 0\\
    \end{array}
  \right]
$$
여기까지는 일반적인 대각 행렬에 대한 정의입니다. SVD를 통해 나온 대각 행렬 Σ는 추가적인 성질을 가지는데, 대각 행렬 Σ의 주대각원소를 행렬 A의 특이값(singular value)라고 하며, 이를![img](https://wikidocs.net/images/page/24949/%ED%8A%B9%EC%9D%B4%EA%B0%921.png)라고 표현한다고 하였을 때 특이값 ![img](https://wikidocs.net/images/page/24949/%ED%8A%B9%EC%9D%B4%EA%B0%921.png)은 내림차순으로 정렬되어 있다는 특징을 가집니다.

아래의 그림은 특이값 12.4, 9.5, 1.3이 내림차순으로 정렬되어져 있는 모습을 보여줍니다.
$$
Σ=
\left[
    \begin{array}{c}
      12.4\ \ 0\ \ 0\\
      0\ \ 9.5\ \ 0\\
      0\ \ 0\ \ 1.3\\
    \end{array}
  \right]
$$

## 2. 절단된 SVD(Truncated SVD)

위에서 설명한 SVD를 풀 SVD(full SVD)라고 합니다. 하지만 LSA의 경우 풀 SVD에서 나온 3개의 행렬에서 일부 벡터들을 삭제시킨 절단된 SVD(truncated SVD)를 사용하게 됩니다. 그림을 통해 이해해보도록 하겠습니다.

![img](https://wikidocs.net/images/page/24949/svd%EC%99%80truncatedsvd.PNG)

절단된 SVD는 대각 행렬 Σ의 대각 원소의 값 중에서 상위값 t개만 남게 됩니다. 절단된 SVD를 수행하면 값의 손실이 일어나므로 기존의 행렬 A를 복구할 수 없습니다. 또한, U행렬과 V행렬의 t열까지만 남깁니다. **여기서 t는 우리가 찾고자하는 토픽의 수를 반영한 하이퍼파라미터값입니다.** 하이퍼파라미터란 사용자가 직접 값을 선택하며 성능에 영향을 주는 매개변수를 말합니다. t를 선택하는 것은 쉽지 않은 일입니다. **t를 크게 잡으면 기존의 행렬 A로부터 다양한 의미를 가져갈 수 있지만, t를 작게 잡아야만 노이즈를 제거할 수 있기 때문입니다.**

이렇게 일부 벡터들을 삭제하는 것을 데이터의 차원을 줄인다고도 말하는데, 데이터의 차원을 줄이게되면 당연히 풀 SVD를 하였을 때보다 직관적으로 계산 비용이 낮아지는 효과를 얻을 수 있습니다.

하지만 계산 비용이 낮아지는 것 외에도 상대적으로 중요하지 않은 정보를 삭제하는 효과를 갖고 있는데, 이는 영상 처리 분야에서는 노이즈를 제거한다는 의미를 갖고 자연어 처리 분야에서는 설명력이 낮은 정보를 삭제하고 설명력이 높은 정보를 남긴다는 의미를 갖고 있습니다. 즉, 다시 말하면 기존의 행렬에서는 드러나지 않았던 심층적인 의미를 확인할 수 있게 해줍니다.



## 3. 잠재 의미 분석(Latent Semantic Analysis, LSA)

기존의 DTM이나 DTM에 단어의 중요도에 따른 가중치를 주었던 TF-IDF 행렬은 단어의 의미를 전혀 고려하지 못한다는 단점을 갖고 있었습니다. LSA는 기본적으로 DTM이나 TF-IDF 행렬에 절단된 SVD(truncated SVD)를 사용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 끌어낸다는 아이디어를 갖고 있습니다. 실습을 통해서 이해해보겠습니다.

| 과일이 | 길고 | 노란 | 먹고 | 바나나 | 사과 | 싶은 | 저는 | 좋아요 |
| :----- | :--- | :--- | :--- | :----- | :--- | :--- | :--- | :----- |
| 문서1  | 0    | 0    | 0    | 1      | 0    | 1    | 1    | 0      |
| 문서2  | 0    | 0    | 0    | 1      | 1    | 0    | 1    | 0      |
| 문서3  | 0    | 1    | 1    | 0      | 2    | 0    | 0    | 0      |
| 문서4  | 1    | 0    | 0    | 0      | 0    | 0    | 0    | 1      |