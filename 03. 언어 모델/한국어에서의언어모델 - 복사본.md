# 한국어에서의 언어 모델

**영어나 기타 언어에 비해서 한국어는 언오 모델로 다음 단어를 예측하기가 훨씬 까다롭습니다.**

## 1. 한국어는 어순이 중요하지 않다.

그래서 이전 단어가 주어졌을떄. 다음 단어가 나타날 활귤을 구해야 하는데 어순이 중요하지 않다는 것은 다음 단어로 어떤 단어든 등장할 수 있다는 의미

① 나는 운동을 합니다 체육관에서.
② 나는 체육관에서 운동을 합니다.
③ 체육관에서 운동을 합니다.
④ 나는 운동을 체육관에서 합니다.

4개의 문장 모두 의미가 통한다. 심지어 주어를 빼도 말이된다. 이렇게 단어 순서를 뒤죽박죽으로 바꾸어놔도 한국어는 의미가 전달 되기 때문에 확률에 기반한 언어 모델이 제대로 다음 단어를 예측하기가 어렵습니다.



## 2. 한국어는 교착어이다.

한국어는 교착어이라는 점이 한국어에서의 언어 모델 작동을 어렵게 만듭니다.

띄어쓰기 단위인 어절 단위로 토큰화를 할 경우에는 문장에서 발생가능한 단어의 수가 굉장히 늘어납니다. - 조사가 있기 때문

- '그녀' ->{그녀가, 그녀를 , 그녀의, 그녀와, 그녀로, 그녀께서}

- **토큰화를 통해 접사나 조사등을 분리하는 것은 중요한 작업이다.**

```python
# 1. 데이터 수집
import codecs
with codecs.open('data2.txt',encoding='utf-8') as f: #문서 로드
    #한줄의 문장이 id, docment label이 '\t'로 분리되어있다.
    data = [문장.split('\t') for 문장 in f.read().splitlines()]
    data=data[1:] #첫번째 feature_names제거
    
# 2. 데이터 전처리(문장토큰화 및 정제)
t_data = [문장[1] for 문장 in data]
#우리는 document 피쳐만이 필요하다.

#3. 데이터 전처리(문장->단어 토큰화)
from konlpy.tag import Okt
from tqdm import tqdm #작업의 진행사항을 알려주는 패키지
from nltk.util import ngrams #ngrams 
tk = Okt()
def tk_f(t_s_data):
    tk_d = ['/'.join(x) for x in tk.pos(t_s_data)] # 각 줄을 품시토큰으로 분리하고, 품사토큰이 ','으로 된걸 ->'/'을 만든다.
    return tk_d
end_data=[]

for t in tqdm(t_data): #반복문을 통한 작업 진행사항 확인
    tk_data = tk_f(t) #/'+형택소로 만들어진 데이터#문자열 데이터 #문장->형태소 문장
    #바이그램을 만들기 위한 SS<문장>SE전처리
    bg=ngrams(tk_data,2,pad_left=True,pad_right=True,left_pad_symbol="SS",right_pad_symbol='SE')#품사 토큰화된 문장->바이그램 토큰화
    end_data+=[t for t in bg] #바이그램 각 토큰을 end_data에 삽입
    #end_data는 문서에 존재하는 모든 형태소 토큰과 'SS'와 'SE'를 가지고 있다.
    
from nltk import ConditionalFreqDist  #문맥별 빈도수 계산 클래스
from nltk.probability import ConditionalProbDist,MLEProbDist #문맥별 조건부 확률 딕셔너리

cfd = ConditionalFreqDist(end_data)
cpd = ConditionalProbDist(cfd, MLEProbDist)

def 정리_생성():
    c = "SS"
    sentence = []
    while True:
        w = cpd[c].generate() #c문맥 다음을 발생

        if w == "SE": #문장의 끝을 의미 'SE'라면 break
            break

        w2 = w.split("/")[0] #형태소의 의미만
        pos = w.split("/")[1] #형태소의 품사

        if c == "SS": #시작토큰 이후 단어라면
            sentence.append(w2.title()) #다음 문자를 대문자로
        elif c in ["`", "\"", "'", "("]: #시작토큰이 해당 특수문자라면
            sentence.append(w2)
        elif w2 in ["'", ".", ",", ")", ":", ";", "?"]: #다음 토큰이 해당 특수문자라면
            sentence.append(w2)
        elif pos in ["Josa", "Punctuation", "Suffix"]: #다음 토큰의 형태소가 다음과 같다면
            sentence.append(w2)
        elif w in ["임/Noun", "것/Noun", "는걸/Noun", "릴때/Noun",
                   "되다/Verb", "이다/Verb", "하다/Verb", "이다/Adjective"]:
            sentence.append(w2)
        else:
            sentence.append(" " + w2) #아니라면 어절이니 띄어쓰기
        c = w

    return "".join(sentence)
```



## **3. 한국어는 띄어쓰기가 제대로 지켜지지 않는다.**

제대로 하지 않아도 의미가 전달되며, 띄어쓰기 규칙 또한 상대적으로 까다로운 언어이기 때문에 자연어 처리를 하는 것에 있어서 한국어 코퍼스는 띄어쓰기가 제대로 지켜지지 않는 경우가 많습니다. 

토큰이 제대로 분리되지 않은채 훈련 데이터로 사용된다면 언어 모델은 제대로 동작하지 않습니다.

