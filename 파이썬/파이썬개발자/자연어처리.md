# 자연어 처리

---

| 분야                                          | 용어                   | 용어풀이                                                     |
| :-------------------------------------------- | ---------------------- | ------------------------------------------------------------ |
| 텍스트 처리                                   | 토큰화                 | 언어를 한 단위의 의미를 가지는 **문자열로 쪼개는 과정**이다. 토큰화에는 문장 토큰화, 단어 토큰화 등이 있다. |
|                                               | 표제어                 | 표제어를 추출하는 기법, 단어 사전에 실린거 대로              |
|                                               | 불용어                 | 큰 의미는 없지만 학습 결과에 유의미한 영향을 미칠 수 있는 토큰들을 이르는 말이다. 전처리 과정에서 없앤다. |
| 임베딩 기본                                   | 임베딩                 | 컴퓨터가 알아 듣고 학습을 할 수 있도록 자연어를 숫자로 바꾸는 작업 |
|                                               | 희소벡터               | 희소 행렬에 속하는 벡터로, 희소 행렬은 대부분의 원소가 0인 벡터 |
|                                               | 원-핫인코딩            | 모든 토큰을 하나의 class로 분리시키는 작업, 각각의 토큰 인덱스에 따라 그 인덱스에 해당 하는 값만 '1'로 켜주고, 나머지는 '0'을 넣어 단어 표현 |
|                                               | 밀집벡터               | 밀집 행렬에 속하는 벡터로, 밀집행렬은 대부분의 원소가 0이 아닌 실수값 |
| 단어  분산표                                  | 분산표현               | '단어의 의미는 주변단어에 의해 형성된다는' 분포 가설이 전재  |
|                                               | 통계 기반 기법         | 토큰 등장 빈도수를 카운트하여 단어의 분산을 표현하는 방식    |
|                                               | ㄴDTM                  | 문서 단어 행렬: 단어 빈도수 기반하여 접근하는 방식, 모든 문서에 등장하는 단어 집합에서 각 문서의 각 토큰의 빈도를 행렬로 표현. 단어의 순서는 고려하지 않는다. |
|                                               | TF-IDF                 | 문서에서 각 단어에 중요 가중치를 부여할때 사용한다. 주로 유사도 구할 때 사용. <br /> TF(단어 빈도수) * IDF(역문서 빈도수) |
|                                               | word2vec               | 단어간 의미적 유사성을 벡터화 하는 것                        |
|                                               | Doc2Vec                | 문서간 의미적 유사성을 벡터화 하는 벡터화 하는               |
| 자연어 처리에서 많이 사용되는 인공신경망 모델 | 자연어처리 모델의 특징 | 언어는 시퀀스 데이터이다. 순서에 의해 전혀 다른 의미가 되기도 하는 것이다. 따라서 시퀀스 데이터를 처리하기에 알맞는 모델이 사용된다. |
|                                               | CNN                    | **합성곱 계층과 풀링 계층을 사용하여 특징을 추출하다. 주로 시그모이드나 렐루와 같은 활성화 함수를 이용하며, 주로 분류 작업에 쓰인다.** |
|                                               | 합성곱 계층            | 필터를 일정 간격으로 이동시키며 합성곱을 통해 특징을 추출한다. |
|                                               | 풀링                   | 데이터 크기를 줄이는 작업으로, Max Pooling, Average Poling이 사용된다. |
|                                               | RNN                    | 문자열의 시퀀스 특징을 고려한 모델로, 전 단계의 정보를 다음 단계에 **hidden_state**로 넘겨, 전 단계의 정보도 고려할 수 있다. |
|                                               | ㄴ기울시 소멸          | 역전파시 그래디언트가 너무 작아져서 학습이 제대로 이뤄지지 않는 경우를 말한다. |
|                                               | LSTM                   | RNN의 장기의존성 문제를 해결하기 위해 cell_state를 지속적으로 가져간다. cell_state는 다음 스텝에 어떠한 정보를 가져갈지를 결정한다. |
|                                               | seq2seq                | 인코더와 디코더로 이루어져잇다. 인코더에서 나온 컨텍스트 벡터를 디코더로 전달하며 정보 전달된다. |
|                                               |                        |                                                              |



손실 함수: 예측값과 실제값의 차이를 비교하기 위한 함수. 회귀(mse)

최적화 함수: 손실함수의 결과값을 최소화 하는 함수

- 경사하강법: 손실함수의 경사를 계산하고, 기울기가 낮은 방향으로 반복적으로 조정함으로서 최적의 파라미터를 찾아가는 방식

활성화 함수: 입력 신호의 총합을 출력신호로 변환하는 함수