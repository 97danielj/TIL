[toc]

# 강화학습 소개

## 머신러닝 속 강화학습

머신러닝

| 지도학습         | 비지도학습         | 강화학습           |
| ---------------- | ------------------ | ------------------ |
| 레이블데이터     | 레이블데이터X      | 결정 프로세스      |
| 즉각적 피드백    | 피드백X            | 보상 시스템        |
| 결과/미래를 예측 | 숨겨진 구조를 찾기 | 일련의 행동을 학습 |

- **Supervised Learning** : 지도학습은 Label이라는 정답 data를 알고 있어서 이로부터 즉각적인 feedback을 받으며 학습하는 것을 말합니다. 현재의 data들을 바탕으로 미래를 예측하거나, 정답이 있는 결과를 맞추는데에 목적이 있습니다. 대표적으로 분류(Classification)를 예로 들 수 있겠네요. K-NN, SVM, Decision Tree 등이 있습니다. 

- **Unsupervised Learning** : 비지도학습은 정답에 해당되는 Label이 없습니다. 따라서 즉각적인 feedback을 받을 수 없습니다. 대표적으로는 Clustering과 같은 방법론들이 비지도학습에 해당합니다.

- **Reinforcement Learning** : 강화학습은 Label은 없지만 환경으로부터 주어진 reward를 통해 action을 학습합니다. 이와 같은 면에서 reward도 label로 보는 견해가 있으나, 환경과 상호작용을 한다는 측면에서 다릅니다.

## 강화학습의 정의

강화학습은 기계 학습의 한 영역이다. 행동시림학에서 영감을 받았으며, 어떤 환경 안에서 정의된 에이전트가 현재의 상태를 인식하며, 선택 가능한 행동들 중 보상을 최대화하는 행동 혹은 행동 순서를 선택하는 방법니다.

환경과 에이전트 사이에서 상태, 행동, 보상을 interaction하며 학습하는 것이라고 개괄적으로 이해할 수 있습니다.

즉, Environment(환경, 이하 Env.)이 Agent에게 특정 상황**(state)**을 주면(Agent가 관찰에 의해 이를 얻게 된다고 볼 수도 있습니다) 

Agent는 그에 대해 반응**(action)**을 하고 Env.은 Agent에게 보상**(reward)**를 주게 됩니다. 이러한 과정으로 Agent는 Env.와 상호작용을 하며 reward를 많이 취할 수 있는 action들을 학습을 하는 것이고, 이를 강화학습이라고 부릅니다.

엄밀히 말씀드리자면, 강화학습의 목적은 reward를 최대화 하는 policy를 찾는 것인데, 이는 차차 다루도록 하겠습니다 :)

Wikipedia의 정의와 마찬가지로 강화학습은 행동심리학에 영향을 받았습니다. 그래서 사람이 무엇을 배우는 방식과 매우 유사하게 학습을 합니다. 우리는 어릴 때부터 자전거를 타고 운전을 하며 컴퓨터를 사용할 수 없었습니다. 우리가 할 줄 아는 모든 것은 **방법을 알고 이것이 익숙해지는 학습의 과정**들이 있었습니다. 강화학습도 마찬가지로 1) Trial and Error, 2) Delayed Rewoard라는 두 가지 특성을 갖습니다.

## Properties of RL

1. Trial and Error

   의미는 그대로 시행착오입니다. 예측된 행동이 아니라, 행동으로 얻은 보상으로 행동의 가치를 판단하고 더 많은 보상을 얻기 위해 배워간다.

2. Delayed Reward

   이 두 번째 특징은 '보상을 받는 시간'과 관련이 있습니다. 강화학습이 적용되는 문제들은 시계열이기 때문에 Agent가 한 행동에 대한 보상이 늦어질 가능성이 있습니다. 또한, 단일 행동에 대한 것이 아닌 여러 행동의 조합으로 보상이 이루어지는 경우도 있기 떄문에 보상에 시간의 개념이 적용됩니다.



## 강화학습 예시

### Atari 'Breakout'(벽돌깨기)

1. Env.로부터 state를 받아

2. 주어진 state에서 어떤 action을 취하면

3. score를 maximize하는 어떤 action을 Trail and Error를 반복하며 학습합니다.

4. 이 터널을 뚫고 그 곳으로 공을 보내는 action이 단일 행동에 대해 reward를 가장 많이 얻으므로

5. 이 action을 계속해서 채택하게 됩니다.

이와 같이 일련의 action들을 연속적으로 채택하게되는 것을 policy라고 합니다.

시행착오를 반복하며 agent가 학습하게된 결과물입니다.



## 마치며

강화학습은 학습을 하는 '방식'이 아닌, 어떻게 강화학습 '문제'인가로 정의됩니다.

강화학습은 MDP로 표현되는 문제를 푸는 것이라고 할 수 있습니다.





