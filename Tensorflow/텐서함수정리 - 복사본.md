[toc]
# 2. TensorFlow Window Datasets

tf.Dataset 객체로 만들려면 Numpy array형태로 넣어주어애 하는데, 이 때 array에는 모두 정수 또는 실수형의 데이터들만 있어야 한다. 데이터의 datetime칼럼처럼 object나 datetime형태로 들어가선 안 된다.

Dataset 클래스 : 데이터 파이프라인을 구성함에 있어 매우 유용한 클래스





## **0. Tensorflow Window 함수들 이해하기**

Tensorflow 윈도우 데이터셋 객체로 만드는 방법을 배우기 전에 몇 가지 배워야 할 함수들이 있다.

```python
import tensorflow as tf
ds_x = tf.data.Dataset.from_tensor_slices(X_train)
ds_x = ds_x.window(size=X_window_size, stride = stride, shift=shift, drop_reminder = True)
ds_x = ds_x.flat_map(lambda x: x.batch(X_window_size))
```

**tf.data.Dataset.from_tensor_slices 함수는 Numpy Array를 입력 받아 Tensor로 바꾸어 주는 역할을 한다.** 구체적인 클래스 이름은 TensorSliceDataset이지만 이것이 어떤 클래스인지에 대해서는 추후에 설명한다. tf.data.Dataset.from_tensor_slices 함수로 반환받은 **TensorSliceDataset 객체를 window라는 함수를 사용해서 윈도우 데이터셋으로 분할**할 수 있다. 자주 사용되는 인자로는 **size, stride, shift, drop_remainder** 가 있다.

이 중 drop_remainder에 대해서만 설명하겠다. 나머지는 아래 예시를 살펴보면서 이해하자. 

- `window`: 그룹화 할 윈도우 크기(갯수)
- `drop_remainder`: 남은 부분을 버릴지 살릴지 여부
- `shift`는 1 iteration당 몇 개씩 이동할 것인지

drop_remainder는 특정 윈도우 사이즈로 짤라낼 때 , 데이터 개수에 따라 마지막 윈도우는 특정 사이즈만큼의 길이가 아닐 수 도 있다. 예를 들어, 총 9개의 데이터가 있는데 윈도우 사이즈를 2라고 한다면 2개, 2개, 2개, 2개, 1개가 될 것이다. **이 때 사이즈가 1개인 마지막 윈도우를 drop 할지 말지를 결정하는 것이 drop_remainder 가 하는 역할**이다. 보통은 True로 설정해서 마지막 남은 윈도우를 날려버리는데, **왜냐하면 마지막 윈도우 사이즈가 다름에도 drop_remainder = False로 설정한다면 딥러닝 모델에 윈도우 데이터를 입력시킬 때 에러를 발생시키기 때문**이다. 모델링을 많이 해보신 분은 아시겠지만 입력으로 넣을 때, 데이터의 shape 아다리(?)를 맞춰주지 못해서 많이 고생한 경험이 있을 것이다.

다음은 **flat_map** 함수이다. 파이썬의 map 함수와 기능이 비슷하다. **차이점은 'flat(평평한)'의 의미처럼, 인자로 넣은 Tensor의 한 차원을 줄여주어 반환**하게 된다. 그리고 **batch 함수 인자에는 윈도우 사이즈와 동일한 값을 넣어준다**. 이 때, **batch 함수에 넣는 인자값은 방금 위에서 size로 정의한 윈도우 사이즈를 얼마만큼의 배치 사이즈로 가져올지를 의미**한다.

예를 들어, 7일을 윈도우 사이즈 즉, size의 값으로 설정했다. 그러면 7일씩 하나의 윈도우로 설정해서 데이터를 가져올 것이다. 그런데 이 때, batch 함수에 만약 2를 집어넣는다면 7일씩 데이터를 또 2일 데이터씩 쪼개서 가져오게 된다. **따라서 보통은 size에 넣는 값과 batch 함수 인자에 넣는 값을 동일**하게 해준다. 즉, 7일을 윈도우 사이즈로 설정했으면 batch 사이즈도 7로 설정해서 한 번에 다 가져오도록 설정한다.



## **1. Window 함수의 size, shift, stride 이해하기**

이번엔 window 함수의 인자에 대해 이해해보자. 먼저 아래와 같은 소스코드가 있다고 가정하자. 참고로 t**ake 함수는 데이터를 몇 개 가져올지를 수행한다.**

```python
# 로그 레벨 3으로 변경해서 경고 표시 안뜨게 하기
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

X_window_size = 1  # 총 1개의 window(총 1개의 row)
stride = 1         # 윈도우내에서 데이터의 간격
shift = 1          # 이전 window와 다음 window 시작점 간의 간격

ds_x = tf.data.Dataset.from_tensor_slices(X_train) #arrry를 tesorslicedataset으로 변환
ds_x = ds_x.window(size=X_window_size, stride=stride, shift=shift, drop_remainder=True)
ds_x = ds_x.flat_map(lambda x: x.batch(X_window_size))

for x in ds_x.take(4):
    print(x.shape)
    print(x)
    print('-'*100)
```

위 소스코드에서 size = 1, shift = 1, stride = 1일 때의 각각 의미를 도식화해보면 아래와 같다.

<img src="https://blog.kakaocdn.net/dn/x0O2h/btrkbT8sN5b/rRzMPmsiU9F6ZdU5GtmfFk/img.png" alt="img" style="zoom: 33%;" />

소스코드에서 size=1로 설정했다. 그 말은 즉, 윈도우 사이즈를 1로 한다는 의미이다. 위 그림에서 A, B라고 되어 있는 부분이 바로 사이즈가 1인 윈도우 2개를 의미한다**. 그리고 A,B 윈도우 간의 간격은 현재 하루 차이로 1일이다. 윈도우 간의 간격을 설정 몇 인지를 설정하는 것이 shift가 하는 역할**이다. 위 예시에서는 stride를 어떤 값으로 설정해도 바뀌지 않는다. 왜냐하면 윈도우 내의 데이터가 1개 밖에 없기 때문이다.  **따라서 stride는 size가 2 이상일 때만 적용**된다.

이번엔 stride를 가시적으로 이해하기 위해서 **size와 shift를 2로 늘린 후, stride가 1일 때와, 2일 때 차이를 비교**해보자. 먼저 stride = 1일 때이다.

<img src="https://blog.kakaocdn.net/dn/AOald/btrj3LYIaSk/LHN6Mk11IME3s8CKMrtdX1/img.png" alt="img" style="zoom:33%;" />

**size = 2이기 때문에 A 윈도우는 2009-01-01, 2009-01-02를 담고 있다.** 이 때, **shift = 2로 설정**했기 때문에 **B 윈도우는 2009-01-03, 2009-01-04를 담고 있다.** 위에서 언급한 것처럼 **shift은 윈도우 간의 시간 차이를 의미**한다고 했다. **A의 윈도우 내 첫 번째 데이터인 2009-01-01과 B 윈도우 내 첫 번째 데이터인 2009-01-03은 2일 차이**임을 알 수 있다. 이것이 바로 shift의 의미다.

이제 **stride = 1**을 살펴보자. **stride는 윈도우 내의 날짜의 간격을 의미**한다. A 윈도우 내에는 2009-01-01, 2009-01-02를 담고 있다. 즉, **stride는 2009-01-01 과 2009-01-02의 시간 차이를 의미**한다. B 윈도우도 마찬가지다. 그렇다면 위 조건에서 stride만 2로 바꾸었을 때는 아래와 같이 변경된다.

<img src="https://blog.kakaocdn.net/dn/cCoc9M/btrj4sc0LVF/HsNuqeUuEzI53PTK6lMmS1/img.png" alt="img" style="zoom:33%;" />

**stride를 2로 변경**했기 때문에 2009-01-03 데이터를 A, B 윈도우가 동시에 공유하고 있다. **A 윈도우는 stride가 2인 것을 지키기 위해 2009-01-01 과 2일 차이가 나는 2009-01-03 데이터를 포함**한다. 이 떄, **shift = 2**이기 때문에 **B 윈도우는 A 윈도우와 간격이 2일** 이어야 한다. 따라서 A 윈도우의 시작날짜인 2009-01-01 에 2일을 더한 2009-01-03일이 B 윈도우의 시작 날짜이다. 그리고 **B 윈도우도 stride = 2를 지키기 위해 2009-01-03일 보다 2일 뒤인 2009-01-05 데이터를 포**함한다.

제 어느 정도 감이 잡혔을까? 그렇다면 만약 **size = 3**, **shift = 3**, **stride = 1**로 설정했다면, 다음과 같은 데이터로 분할 될 것이다.

<img src="https://blog.kakaocdn.net/dn/bFQKUr/btrj89cWrsD/VeCB9o4eA9SMerse9qx3b1/img.png" alt="img" style="zoom:33%;" />

- 윈도우 사이즈 : 3
- 윈도우 간 간격 : 3
- 윈도우 내의 데이터 간격 : 1

<img src="https://blog.kakaocdn.net/dn/V5L1F/btrkcqFfu7Z/DmqDWsjLXrlNd3x9VpWIhK/img.png" alt="img" style="zoom:33%;" />

이러한 방식으로 y_train 변수에 대해서도 원하는 예측 유형에 맞게 윈도우 데이터셋을 만들어주면 된다. 따라서 X, y에 대해 윈도우 데이터셋을 만드는 것을 함수화하면 아래와 같이 할 수 있다. **주의할 점은 문제 유형에 따라 X, y 윈도우 사이즈가 다를 수 있기 때문에 X, y의 각 window 함수로 넣어주는 size, shift, stride 값은 개별 인자로 넣어주도록 하자.**

```python
import tensorflow as tf  # 2.7.0 version

def window_dataset(X, y, X_size, y_size, X_shift, y_shift, X_stride, y_stride, batch_size):
	#numpy array x(input)를 Dataset으로 변환
    ds_x = tf.data.Dataset.from_tensor_slices(X)
    #개별 윈도우 셋 생성
    ds_x = ds_x.window(size=X_size, stride=X_stride, shift=X_shift, drop_remainder=True)
    #인자로 넣은 텐서의 한 차원을 줄여서 반환. 윈도우셋의 배치 사이즈 만큼 반환
    ds_x = ds_x.flat_map(lambda x: x.batch(X_size))
    
    ds_y = tf.data.Dataset.from_tensor_slices(y) 
    ds_y = ds_y.window(size=y_size, stride=y_stride, shift=y_shift, drop_remainder=True)
    ds_y = ds_y.flat_map(lambda y: y.batch(y_size))
    
    ds = tf.data.Dataset.zip((ds_x, ds_y))
    return ds.batch(batch_size).prefetch(1)

tf_dataset = window_dataset(X_train, y_train, X_size, y_size,
                            X_shift, y_shift, X_stride, y_stride, batch_size)
```



## **2. 단일스텝 예측**

**이제 위에서 배운 window 함수를 사용해서 단일스텝 예측을 위한 윈도우 데이터셋을 만들어보자.** 먼저 단일스텝 예측이 어떤 방식으로의 예측인지를 도식화해서 알아보자. 참고로 단일스텝 개념과 추후에 소개할 다중스텝 개념은 [Tensorflow 시계열 예측 공식 문서](https://www.tensorflow.org/tutorials/structured_data/time_series?hl=ko#다중_스텝_모델)에서 설명하는 것과 동일하다.

<img src="https://blog.kakaocdn.net/dn/bjzOlU/btrku2EHmUU/CEV3i0c7LFVy1C3lOmEUT0/img.png" alt="img" style="zoom: 50%;" />



위 그림의 2가지 데이터프레임 유형은 **단일스텝 예측의 전형적인 유형**이라고 할 수 있다. 첫 번째 그림부터 살펴보자. 0번째 데이터의 Feature들로 1번째 레이블을 예측하고, 1번째 데이터로 2번째 레이블을 예측하고, ... **반복적으로 현재 타임스텝의 1개 데이터를 가지고 다음의 단일스텝 1개를 예측**한다. 이러한 유형을 윈도우 데이터셋으로 만들어보는 소스코드는 아래와 같다.(소스코드에 사용된 window_dataset 함수는 위에서 소개한 함수와 동일하다)

```python
def window_dataset(X, y, X_size, y_size, X_shift, y_shift, X_stride, y_stride, batch_size):

    ds_x = tf.data.Dataset.from_tensor_slices(X)
    ds_x = ds_x.window(size=X_size, stride=X_stride, shift=X_shift, drop_remainder=True)
    ds_x = ds_x.flat_map(lambda x: x.batch(X_size))
    
    ds_y = tf.data.Dataset.from_tensor_slices(y)
    ds_y = ds_y.window(size=y_size, stride=y_stride, shift=y_shift, drop_remainder=True)
    ds_y = ds_y.flat_map(lambda y: y.batch(y_size))
    
    ds = tf.data.Dataset.zip((ds_x, ds_y))
    return ds.batch(batch_size).prefetch(1)

# X에 대한 윈도우 함수 설정값
X_size = 1
X_shift = 1
X_stride = 1
# y에 대한 윈도우 함수 설정값
y_size = 1
y_shift = 1
y_stride = 1

batch_size = 1

tf_dataset = window_dataset(X_train, y_train[1:], X_size, y_size,
                            X_shift, y_shift, X_stride, y_stride, batch_size)

# 데이터 shape, 미리보기로 체크
for x, y in tf_dataset.take(3):
    print('X:', x.shape)
    print(x)
    print()
    print('Y:', y.shape)
    print(y)
    print('-'*100)
```

주의할 점은 **y_train을 설정해줄 때, 한 단계 lag된 값을 넣어주어야 하기 때문에 슬라이싱으로 [1:] 하는 것을 잊지 말자.** 아래는 위 소스코드에서 take(3) 으로 윈도우 데이터셋을 미리보기하고 실제 데이터프레임과 일치하는지 비교한 결과다. 

<img src="https://blog.kakaocdn.net/dn/bc59rV/btrkyPRO2gL/vpki1PKCjip48Fw0Deqqv1/img.png" alt="img" style="zoom:33%;" />

첫 번째 데이터인 **20090101 이라는 데이터의 레이블로는 -4가 들어가 있는 것**을 볼 수 있다. 이를 하단 셀의 test 라는 데이터프레임 출력화면을 보면 **-4가 20090102 날짜의 y 값임**을 알 수 있다. 예상대로 잘 분할되었다.

 

다음은 밑에 있는 두 번째 그림에 있는 **단일스텝 예측 유형**이다. 주의할 점은 X, y의 윈도우 사이즈(size 값)가 커졌다고 해서 다중스텝을 의미하지 않는다. **그림을 보면 알겠지만 단지 첫 번째 유형보다 단순히 size만 커진 것일 뿐, 하나의 데이터(데이터프레임의 하나의 row)가 들어가서 다음 스텝의 레이블 하나를 예측하는 것은 동일**하다. Tensorflow 공식문서에서는 이에 대해 아래와 같은 그림으로 설명해준다.(아래 그림을 밑에서 소개할 다중스텝과 비교해보자)


 <img style="max-height: 50%; max-width: 50%; zoom: 50%;" src="https://blog.kakaocdn.net/dn/qP0yF/btrkvIePPap/dKpC2ex1ONYWIH5KgyVwkk/img.png">

따라서 두 번째 그림을 윈도우 데이터셋으로 분할하는 방법은 아래와 같다.

```python
def window_dataset(X, y, X_size, y_size, X_shift, y_shift, X_stride, y_stride, batch_size):

    ds_x = tf.data.Dataset.from_tensor_slices(X)
    ds_x = ds_x.window(size=X_size, stride=X_stride, shift=X_shift, drop_remainder=True)
    ds_x = ds_x.flat_map(lambda x: x.batch(X_size))
    
    ds_y = tf.data.Dataset.from_tensor_slices(y)
    ds_y = ds_y.window(size=y_size, stride=y_stride, shift=y_shift, drop_remainder=True)
    ds_y = ds_y.flat_map(lambda y: y.batch(y_size))
    
    ds = tf.data.Dataset.zip((ds_x, ds_y))
    return ds.batch(batch_size).prefetch(1)

# X에 대한 윈도우 함수 설정값 / 위의 window_dataset에서 size만 증가
X_size = 3
X_shift = 1
X_stride = 1
# y에 대한 윈도우 함수 설정값
y_size = 3
y_shift = 1
y_stride = 1

batch_size = 1

#lag = 3
tf_dataset = window_dataset(X_train, y_train[3:], X_size, y_size,
                            X_shift, y_shift, X_stride, y_stride, batch_size)

# 데이터 shape, 미리보기로 체크
for x, y in tf_dataset.take(3):
    print('X:', x.shape)
    print(x)
    print()
    print('Y:', y.shape)
    print(y)
    print('-'*100)
```



## **3. 다중스텝 예측**

이전에 알아본 **단일스텝 예측은 하나씩 데이터를 넣어서 시간적인 특성을 파악하지 못하는 단점**이 있다. 그래서 우리는 아래와 같은 유형처럼 **시간적인 특성을 넣어줄 수 있는 다중스텝 예측**을 한다고 가정해보자.

<img src="https://blog.kakaocdn.net/dn/yugzK/btrkweR5DyK/kB4xmJOr8GzTAbq614RF1k/img.png" alt="img" style="zoom: 50%;" />

**위 두 그림의 큰 차이점은 1개의 y값을 예측하는지, 2개 이상의 연속적인 y값을 예측하는 지의 차이**이다. 그리고 **단일스텝 유형과의 큰 차이점은 윈도우를 구성할 때, 윈도우 간의 간격인 shift를 늘려줌으로써 모델이 데이터의 시간적인 특성을 파악**할 수 있도록 했다. 아래 그림을 보면서 위에서 보았던 단일스텝일 때의 그림과 비교하면 차이점을 알 수 있을 것이다.

<img src="https://blog.kakaocdn.net/dn/b5bkn4/btrkuHAKNRC/QMY0XGZv9K88AVKPGcnMmk/img.png" alt="img" style="zoom:50%;" />

위 그림의 첫 번째 유형을 윈도우 데이터셋으로 분할하는 소스코드는 아래와 같다.

```python
def window_dataset(X, y, X_size, y_size, X_shift, y_shift, X_stride, y_stride, batch_size):

    ds_x = tf.data.Dataset.from_tensor_slices(X)
    ds_x = ds_x.window(size=X_size, stride=X_stride, shift=X_shift, drop_remainder=True)
    ds_x = ds_x.flat_map(lambda x: x.batch(X_size))
    
    ds_y = tf.data.Dataset.from_tensor_slices(y)
    ds_y = ds_y.window(size=y_size, stride=y_stride, shift=y_shift, drop_remainder=True)
    ds_y = ds_y.flat_map(lambda y: y.batch(y_size))
    
    ds = tf.data.Dataset.zip((ds_x, ds_y))
    return ds.batch(batch_size).prefetch(1)

# X에 대한 윈도우 함수 설정값
X_size = 3
X_shift = 3
X_stride = 1
# y에 대한 윈도우 함수 설정값
y_size = 1
y_shift = 3
y_stride = 1

batch_size = 1

tf_dataset = window_dataset(X_train, y_train[3:], X_size, y_size,
                            X_shift, y_shift, X_stride, y_stride, batch_size)

# 데이터 shape, 미리보기로 체크
for x, y in tf_dataset.take(3):
    print('X:', x.shape)
    print(x)
    print()
    print('Y:', y.shape)
    print(y)
    print('-'*100)
```

다음은 두 번째 그림에 해당하는 y값을 연속적으로 2개 예측하는 유형에 대한 소스코드이다.

```python
def window_dataset(X, y, X_size, y_size, X_shift, y_shift, X_stride, y_stride, batch_size):

    ds_x = tf.data.Dataset.from_tensor_slices(X)
    ds_x = ds_x.window(size=X_size, stride=X_stride, shift=X_shift, drop_remainder=True)
    ds_x = ds_x.flat_map(lambda x: x.batch(X_size))
    
    ds_y = tf.data.Dataset.from_tensor_slices(y)
    ds_y = ds_y.window(size=y_size, stride=y_stride, shift=y_shift, drop_remainder=True)
    ds_y = ds_y.flat_map(lambda y: y.batch(y_size))
    
    ds = tf.data.Dataset.zip((ds_x, ds_y))
    return ds.batch(batch_size).prefetch(1)

# X에 대한 윈도우 함수 설정값
X_size = 3
X_shift = 3
X_stride = 1
# y에 대한 윈도우 함수 설정값
y_size = 2
y_shift = 3
y_stride = 1

batch_size = 1

tf_dataset = window_dataset(X_train, y_train[3:], X_size, y_size,
                            X_shift, y_shift, X_stride, y_stride, batch_size)

# 데이터 shape, 미리보기로 체크
for x, y in tf_dataset.take(3):
    print('X:', x.shape)
    print(x)
    print()
    print('Y:', y.shape)
    print(y)
    print('-'*100)
```

지금까지 시계열 예측을 위해 Tensorflow의 tf.Dataset 객체를 활용한 윈도우 데이터셋을 만들어 보는 방법에 대해 알아보았다. 위 코드를 기반으로 윈도우 데이터셋을 구축하고 사용자가 원하는 것에 따라 SimpleRNN, LSTM과 같은 순환신경망 모델들을 추가해 모델링을 진행할 수 있다. 다음 포스팅에서는 위 방법을 기반으로 간단하게 순환신경망을 모델링하고 학습시키는 과정까지 진행해보도록 하자.
