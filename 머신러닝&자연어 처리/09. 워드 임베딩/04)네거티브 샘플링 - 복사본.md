[toc]

# 04) 네거티브 샘플링을 이용한 Word2Vec 구현(Skip-Gram with Negative Sampling, SGNS)

네거티브 샘플링(Negative Sampling)을 사용하는 Word2Vec을 직접 케라스(Keras)를 통해 구현해봅시다.

## 1. 네거티브 샘플링(Negative Sampling)

Word2Vec의 출력층에서는 소프트맥스 함수를 지난 단어 집합 크기의 벡터와 실제값인 원-핫 벡터와의 오차를 구하고 이로부터 임베딩 테이블에 있는 **모든 단어에 대한 임베딩 벡터 값을 업데이트**합니다. 만약 단어 집합의 크기가 수만 이상에 달한다면 이 작업은 굉장히 무거운 작업이므로, Word2Vec은 꽤나 학습하기에 무거운 모델이 됩니다.

Word2Vec은 역전파 과정에서 모든 단어의 임베딩 벡터값의 업데이트를 수행하지만, 만약 현재 집중하고 있는 중심 단어와 주변 단어가 '강아지'와 '고양이', '귀여운'과 같은 단어라면, 사실 이 단어들과 별 연관 관계가 없는 '돈가스'나 '컴퓨터'와 같은 수많은 단어의 임베딩 벡터값까지 업데이트하는 것은 비효율적입니다.

네거티브 샘플링은 Word2Vec이 학습 과정에서 전체 단어 집합이 아니라 **일부 단어 집합에만 집중할 수 있도록 하는 방법**입니다. 가령, 현재 집중하고 있는 주변 단어가 '고양이', '귀여운'이라고 해봅시다. 여기에 '돈가스', '컴퓨터', '회의실'과 같은 단어 집합에서 무작위로 선택된 주변 단어가 아닌 단어들을 일부 가져옵니다. 이렇게 하나의 중심 단어에 대해서 전체 단어 집합보다 훨씬 작은 단어 집합을 만들어놓고 마지막 단계를 이진 분류 문제로 변환합니다. 주변 단어들을 긍정(positive), 랜덤으로 샘플링 된 단어들을 부정(negative)으로 레이블링한다면 **이진 분류 문제를 위한 데이터셋**이 됩니다. 이는 기존의 단어 집합의 크기만큼의 선택지를 두고 다중 클래스 분류 문제를 풀던 Word2Vec보다 훨씬 연산량에서 효율적입니다.

## 2. 네거티브 샘플링 Skip-Gram(Skip-Gram with Negative Sampling, SGNS)

앞서 배운 Skip-gram을 상기해봅시다.

![img](https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC1.PNG)

Skip-gram은 중심 단어로부터 주변 단어를 예측하는 모델이었습니다. 위와 같은 문장이 있다고 한다면, Skip-gram은 중심 단어 cat으로부터 주변 단어 The, fat, sat, on을 예측합니다. 기존의 Skip-gram 모델을 일종의 주황 박스로 생각해본다면, 아래의 그림과 같이 입력은 중심 단어, 모델의 예측은 주변 단어인 구조입니다.

![img](https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC1-1.PNG)

하지만 네거티브 샘플링을 사용하는 Skip-gram(Skip-Gram with Negative Sampling, SGNS) 이하 SGNS는 이와는 다른 접근 방식을 취합니다. SGNS는 다음과 같이 중심 단어와 주변 단어가 모두 입력이 되고, 이 두 단어가 실제로 윈도우 크기 내에 존재하는 이웃 관계인지 그 확률을 예측합니다.

![img](https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC1-2.PNG)

기존의 Skip-gram 데이터셋을 SGNS의 데이터셋으로 바꾸는 과정을 봅시다.

![img](https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC3.PNG)

위의 그림에서 좌측의 테이블은 기존의 Skip-gram을 학습하기 위한 데이터셋입니다. Skip-gram은 기본적으로 중**심 단어를 입력, 주변 단어를 레이블**로 합니다. 하지만 SGNS를 학습하고 싶다면, 이 데이터셋을 우측의 테이블과 같이 수정할 필요가 있습니다. 우선, 기존의 Skip-gram 데이터셋에서 중심 단어와 주변 단어를 각각 입력1, 입력2로 둡니다. 이 둘은 실제로 윈도우 크기 내에서 이웃 관계였므로 레이블은 1로 합니다. 이제 레이블이 0인 샘플들을 준비할 차례입니다.

![img](https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC4.PNG)

실제로는 입력1(중심 단어)와 주변 단어 관계가 아닌 단어들을 입력2로 삼기 위해서 단어 집합에서 랜덤으로 선택한 단어들을 입력2로 하고, 레이블을 0으로 합니다. 이제 이 데이터셋은 **입력1과 입력2가 실제로 윈도우 크기 내에서 이웃 관계인 경우에는 레이블이 1, 아닌 경우에는 레이블이 0인 데이터셋이 됩니다.** 그리고 이제 두 개의 임베딩 테이블을 준비합니다. 두 임베딩 테이블은 **훈련 데이터의 단어 집합의 크기**를 가지므로 크기가 같습니다.

![img](https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC5.PNG)

두 테이블 중 하나는 **입력 1인 중심 단어의 테이블 룩업을 위한 임베딩 테이블**이고, 하나는 **입력 2인 주변 단어의 테이블 룩업을 위한 임베딩 테이블**입니다. 각 단어는 각 임베딩 테이블을 테이블 룩업하여 임베딩 벡터로 변환됩니다.

![img](https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC6.PNG)

각 임베딩 테이블을 통해 테이블 룩업하여 임베딩 벡터로 변환되었다면 그 후의 연산은 매우 간단합니다.

![img](https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC7.PNG)

중심 단어와 주변 단어의 **내적**값을 이 모델의 예측값으로 하고, 레이블과의 오**차로부터 역전파**하여 중심 단어와 주변 단어의 임베딩 벡터값을 업데이트합니다. 학습 후에는 좌측의 임베딩 행렬을 임베딩 벡터로 사용할 수도 있고, 두 행렬을 더한 후 사용하거나 두 행렬을 연결(concatenate)해서 사용할 수도 있습니다. 아래의 실습에서는 좌측의 행렬을 사용하는 방식을 택했습니다.

