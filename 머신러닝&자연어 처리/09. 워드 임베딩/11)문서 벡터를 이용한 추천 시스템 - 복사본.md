# 09-11 문서 벡터를 이용한 추천 시스템(Recommendation System using Document Embedding)

문서들을 고정된 길이의 벡터로 변환한다면 벡터 간 비교로 문서들을 서로 비교할 수 있습니다. 각 문서를 **문서 벡터로 변환하는 방법은** 이미 구현된 패키지인 Doc2Vec이나 Sent2Vec 등을 사용하여 학습하는 방법도 존재하지만, 단어 벡터를 얻은 뒤 문서에 존재하는 단어 벡터들의 평균을 문서 벡터로 간주할 수 있습니다. 이번에는 문서 내 각 단어들을 Word2Vec을 통해 단어 벡터로 변환하고, 이들의 평균으로 문서 벡터를 얻어 선호하는 도서와 유사한 도서를 찾아주는 도서 추천 시스템을 만들어보겠습니다.

## 1. 데이터 로드

이번 실습에서 사용할 데이터는 책의 이미지와 책의 줄거리를 크롤링한 데이터로 아래의 링크에서 다운로드 할 수 있습니다.

데이터 다운로드 링크 : https://drive.google.com/file/d/15Q7DZ7xrJsI2Hji-WbkU9j1mwnODBd5A/view?usp=sharing

```python
import urllib.request
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import requests
import re
from PIL import Image
from io import BytesIO
from nltk.tokenize import RegexpTokenizer
import nltk
from gensim.models import Word2Vec
from gensim.models import KeyedVectors
from nltk.corpus import stopwords
from sklearn.metrics.pairwise import cosine_similarity
```

데이터를 데이터프레임으로 로드하고 전체 문서의 수를 출력해봅시다.

```python
urllib.request.urlretrieve("https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/data.csv", filename="data.csv")
df = pd.read_csv("data.csv")
print('전체 문서의 수 :',len(df))
전체 문서의 수 : 2382
```

상위 5개의 행만 출력해봅시다.

```python
df[:5]
```

![img](https://wikidocs.net/images/page/102705/%EC%A0%80%EC%9E%A5.PNG)

불필요한 열들이 존재하는데, 여기서는 줄거리에 해당하는 열인 'Desc 열'이 중요합니다. 해당 열에 있는 데이터에 대해서 Word2Vec을 학습하기 위해 전처리를 진행합니다. 해당 열에 대해서 전처리를 수행하고 'cleaned'라는 열에 저장해봅시다.

```python
def _removeNonAscii(s):
    return "".join(i for i in s if  ord(i)<128)

def make_lower_case(text):
    return text.lower()

def remove_stop_words(text):
    text = text.split()
    stops = set(stopwords.words("english"))
    text = [w for w in text if not w in stops]
    text = " ".join(text)
    return text

def remove_html(text):
    html_pattern = re.compile('<.*?>')
    return html_pattern.sub(r'', text)

def remove_punctuation(text):
    tokenizer = RegexpTokenizer(r'[a-zA-Z]+')
    text = tokenizer.tokenize(text)
    text = " ".join(text)
    return text

df['cleaned'] = df['Desc'].apply(_removeNonAscii)
df['cleaned'] = df.cleaned.apply(make_lower_case)
df['cleaned'] = df.cleaned.apply(remove_stop_words)
df['cleaned'] = df.cleaned.apply(remove_punctuation)
df['cleaned'] = df.cleaned.apply(remove_html)
```

상위 5개의 행만 출력합니다.

```python
df['cleaned'][:5]
0    know power shifting west east north south pres...
1    following success accidental billionaires mone...
2    tap power social software networks build busin...
3    william j bernstein american financial theoris...
4    amazing book joined steve jobs many akio morit...
Name: cleaned, dtype: object
```

전처리 과정에서 빈 값이 생긴 행이 있을 수 있습니다. 빈 값이 생긴 행이 있다면, nan 값으로 변환 후에 해당 행을 제거합니다.

```python
df['cleaned'].replace('', np.nan, inplace=True)
df = df[df['cleaned'].notna()]
print('전체 문서의 수 :',len(df))
전체 문서의 수 : 2381
```

전체 문서의 수가 1개 줄어들었습니다. 토큰화를 수행하여 corpus라는 리스트에 저장합니다. 해당 리스트 corpus를 통해 Word2Vec을 훈련할 것입니다.

```python
corpus = []
for words in df['cleaned']:
    corpus.append(words.split())
```

## 2. 사전 훈련된 워드 임베딩 사용하기

Word2Vec을 처음부터 학습할 수도 있겠지만, 데이터가 충분하지 않은 상황에서 사전 훈련된 워드 임베딩을 단어 벡터의 초기값으로 사용하여 성능을 높일 수 있습니다. 여기서는 사전 훈련된 Word2Vec을 로드하고 초기 단어 벡터값으로 사용합니다.

```python
urllib.request.urlretrieve("https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz", \
                           filename="GoogleNews-vectors-negative300.bin.gz")

word2vec_model = Word2Vec(size = 300, window=5, min_count = 2, workers = -1)
word2vec_model.build_vocab(corpus)
word2vec_model.intersect_word2vec_format('GoogleNews-vectors-negative300.bin.gz', lockf=1.0, binary=True)
word2vec_model.train(corpus, total_examples = word2vec_model.corpus_count, epochs = 15)
```

## 3. 단어 벡터의 평균 구하기

각 문서에 존재하는 단어들의 벡터값의 평균을 구하여 해당 문서의 벡터값을 연산해봅시다.

```python
def get_document_vectors(document_list):
    document_embedding_list = []

    # 각 문서에 대해서
    for line in document_list:
        doc2vec = None
        count = 0
        for word in line.split():
            if word in word2vec_model.wv.vocab:
                count += 1
                # 해당 문서에 있는 모든 단어들의 벡터값을 더한다.
                if doc2vec is None:
                    doc2vec = word2vec_model[word]
                else:
                    doc2vec = doc2vec + word2vec_model[word]

        if doc2vec is not None:
            # 단어 벡터를 모두 더한 벡터의 값을 문서 길이로 나눠준다.
            doc2vec = doc2vec / count
            document_embedding_list.append(doc2vec)

    # 각 문서에 대한 문서 벡터 리스트를 리턴
    return document_embedding_list
document_embedding_list = get_document_vectors(df['cleaned'])
print('문서 벡터의 수 :',len(document_embedding_list))
문서 벡터의 수 : 2381
```

## 4. 추천 시스템 구현하기

각 문서 벡터 간의 코사인 유사도를 구합니다.

```python
cosine_similarities = cosine_similarity(document_embedding_list, document_embedding_list)
print('코사인 유사도 매트릭스의 크기 :',cosine_similarities.shape)
코사인 유사도 매트릭스의 크기 : (2381, 2381)
```

선택한 책에 대해서 코사인 유사도를 이용하여 가장 줄거리가 유사한 5개의 책을 찾아내는 함수를 만듭니다.

```python
def recommendations(title):
    books = df[['title', 'image_link']]

    # 책의 제목을 입력하면 해당 제목의 인덱스를 리턴받아 idx에 저장.
    indices = pd.Series(df.index, index = df['title']).drop_duplicates()    
    idx = indices[title]

    # 입력된 책과 줄거리(document embedding)가 유사한 책 5개 선정.
    sim_scores = list(enumerate(cosine_similarities[idx]))
    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)
    sim_scores = sim_scores[1:6]

    # 가장 유사한 책 5권의 인덱스
    book_indices = [i[0] for i in sim_scores]

    # 전체 데이터프레임에서 해당 인덱스의 행만 추출. 5개의 행을 가진다.
    recommend = books.iloc[book_indices].reset_index(drop=True)

    fig = plt.figure(figsize=(20, 30))

    # 데이터프레임으로부터 순차적으로 이미지를 출력
    for index, row in recommend.iterrows():
        response = requests.get(row['image_link'])
        img = Image.open(BytesIO(response.content))
        fig.add_subplot(1, 5, index + 1)
        plt.imshow(img)
        plt.title(row['title'])
```

좋아하는 책 제목을 입력으로 넣으면 해당 책의 문서 벡터(줄거리 벡터)와 유사한 문서 벡터값을 가진 책들을 추천해줍니다. 책 제목과 표지를 출력합니다.

```python
recommendations("The Da Vinci Code")
```

![img](https://wikidocs.net/images/page/102705/%EC%98%81%ED%99%94.PNG)

다빈치 코드는 작가 댄 브라운의 작품입니다. 추천되는 작품들 또한 5개 중 3개가 댄 브라운의 작품들이 추천됨을 확인할 수 있습니다. 이번에는 아가사 크리스티의 애크로이드 살인사건과 유사한 도서를 추천받아 봅시다.

```python
recommendations("The Murder of Roger Ackroyd")
```

![img](https://wikidocs.net/images/page/102705/%EC%98%81%ED%99%942.PNG)

애크로이드 살인사건은 미스터리 스릴러 소설인데 이와 유사한 소설들이 추천되는 것을 확인할 수 있습니다.