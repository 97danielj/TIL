[toc]

# 데이터 불균형

평생 개만 봐온 사람에게 늑대를 보여주면 개라고 답할 확률이 높겠죠. 머신러닝도 마찬가지입니다. 데이터가 수 십만 개가 있다고 한들, 다른 클래스에 비해 현저히 적은 클래스가 있다면 해당 클래스의 (분류 문제라면) 분류는 어려워집니다. 극단적인 예를 들면 늑대 사진 100장과 개 사진 10,000장을 학습시키면 사람도 구분을 못하는데 기계는 더더욱 개와 늑대를 구분할 수 없습니다. 이런 불균형 데이터 문제를 해결하기 위해 Data Augmentation, Oversampling, Undersampling 등의 방법이 나와있습니다.

<img src="https://k.kakaocdn.net/dn/vk8Su/btrmKh64OOQ/nz8CKTcgB5KOm11RNFrov1/img.jpg" alt="img" style="zoom:50%;" />

모든 지도학습 머신러닝 모델은 불균형 데이터에 좋은 성능을 보일 수 없습니다. 자연어처리 분야에서 한 획을 그었다고 평가받는 BERT(Bidirectional Encoder Representations from Transformers)도 마찬가지입니다. 무적처럼 보였던 BERT도 불균형 데이터 문제에는 당연하다는 듯이 낮은 성능을 보여줍니다.

> 여러 가지 방법으로 데이터 불균형 문제를 해결할 수 있다면 BERT의 성능이 좋아질까?
> 좋아진다면 얼마나 좋아질까?



## 1. 불균형 처리의 목적

불균형 데이터 처리(Data Augmentation, Oversampling, Undersampling 등)가 BERT에도 얼마나 성능 향상이 있는가를 처리하지 않은 모델과 비교하는 것입니다. 즉, 불균형 데이터 처리 BERT 모델과 미처리 BERT 모델의 비교입니다. 뒤에 설명한 Random Swaping과 Random Deletion을 적용하여 한국어 BERT 중 KorBERT를 통한 불균형 데이터 성능 향상을 비교한 논문*이 있습니다. 이뿐 아니라 저희가 선정한 방법들에 대하여 얼마나 큰 성능 향상을 보이는지 확인하고 싶었습니다. 여러 방법을 비교하는 것이 아닌 저희가 선택한 한 가지 방법만 비교하는 것인 만큼, 방법 선정에 있어 나름의 근거를 가지고 신중을 기해야겠죠.

![img](https://k.kakaocdn.net/dn/ce8WV9/btrmEF1lmYt/Nn9C7q3kr3kBuDfKsQlIzK/img.png)

선정한 데이터셋은 영어 댓글의 악성 댓글 데이터셋입니다. 챕터 '4. Dataset'에서 더 자세히 설명하겠지만, Kaggle의 영어 악성 댓글 분류 챌린지 데이터셋**을 가져왔습니다. 해당 데이터셋은 일반 댓글과 악성 댓글의 비율이 약 9:1 정도로 분포되어 있어 해당 프로젝트에 적합하다는 판단입니다. 이 데이터셋으로 불균형 데이터 문제를 처리하고 BERT 모델에 Fine-tuning 해보겠습니다.

[Toxic Comment Classification Challenge | Kaggle](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview/description)

## 2. Notion: 데이터 불균형과 BERT

이번 프로젝트에서 가장 큰 두 가지의 개념은 데이터 불균형과 BERT입니다. 

### 2-1. Notion : 데이터 불균형이란

딥러닝 프로젝트를 진행해본 사람이라면 한 번쯤 불균형 데이터에대해 들어본적이 있을 것입니다. 분류 문제를 해결할 때 데이터의 분포를 가장 먼저 확인하게 됩니다. 이때 예측해야 하는 결과값이 100:1, 400:1,… 정도로 불균형한 분포를 띄고 있는 데이터들을 불균형 데이터라고 합니다. 아래의 그림은 저희가 선정한 불균형 데이터에 대한 그래프입니다.

![img](https://k.kakaocdn.net/dn/MoI7W/btrmY5Fi7eK/isRXzDDTSnCBz8fmPqg0VK/img.png)

이런 불균형 데이터 그대로 예측을 진행하면 과대적합 문제의 발생 가능성이 높아집니다. 데이터가 불균형하면 분포도가 높은 클래스에 가중치를 많이 두기 때문에 모델 자체에서 예측 성능을 높게 평가합니다. 그렇기 때문에 불균형 문제를 그대로 놔두면 accuracy는 높아질 수 있지만 분포가 작은 값에 대한 precision은 낮은 과대적합 문제가 발생하는 것입니다.

데이터의 불균형성을 해결하는 방법은 다양하지만, 저희는 그중에서 세 가지 방법에 대해서 간단히 언급하겠습니다.

1. SMOTE(Synthetic Minority Oversampling Technique)

​	SMOTE는 오버샘플링 방법 중 하나로, 소수의 클래스에 속하는 데이터 주변에 원본데이터와 동일하지 않으면서 소수의 클래스에 해당하는 가상의 데이터를 생성하는 방법입니다.

	2. Focal loss

​	Focal loss는 Object Detection에서 학습 중 클래스 불균형 문제를 손실 함수로 해결하기 위해 고안된 방법입니다. Cross entropy의 클래스 불균형 문제는 백그라운드로 분류될 수 있는 easy negative가 대부분이어서 학습에 비효율적입니다. Focal loss는 Cross Entropy의 이런 클래스 불균형 문제를 개선해, 어렵거나 쉽게 잘못 분류되는 케이스에 더 큰 가중치를 줍니다. Focal loss에 대해 간단히 요약하자면, 잘 찾은 class의 경우에는 loss를 적게 줘 loss 갱신을 거의 하지 못하게 하고, 잘 찾지 못한 class의 경우 loss를 크게 줘서 loss 갱신을 크게 하는 것입니다. 결론적으로 잘 찾지 못한 class에 대해 더 집중해서 학습하도록 하는 방법이라고 볼 수 있습니다.

3.  EDA(Easy Data Augmentation)

​	EDA는 학습 데이터가 부족하거나, 불균형 문제가 발생했을 때, 현재 보유하고 있는 데이터를 변형시켜 데이터의 양을 늘리는 기법입니다.

```tex
1. SR(Synonym Replacement, 동의어 교체): 문장에서 랜덤으로 stop words가 아닌 단어들 중 n개를 선택해 임의로 선택한 동의어들 중 하나로 바꿈
2. RI(Random Insertion, 무작위 삽입): stop word를 제외한 나머지 단어들 중, 랜덤으로 단어를 선택하여 동의어를 임의어로 정하고, 이를 각 문장 내에 임의의 자리에 넣음
3. RS(Random Swap, 무작위 교체): 각 문장에서 무작위로 두 단어를 선택해 그 위치를 바꿈
4. RD(Random Deletion, 무작위 삭제): 각 문장 내에서 랜덤하게 단어를 선택해 이를 삭제함
```

### 2-2. Notion2: BERT도 알려줘!

 아까부터 Team.AK가 강조하는 BERT란 무엇일까요? 버트는 사실 알 만한 분들은 아실 1969년부터 지금까지 방영 중인 세서미 스트리트(Sesame Street)의 캐릭터입니다. 흔히 말하는 '노잼 컨셉'이 역설적으로 개그 캐릭터가 되어버린 세서미 스트리트의 버트처럼 구글 BERT도 처음에는 재미없어 보이지만 알면 알수록 재미있는 모델입니다. 이런 특성까지 비슷한 건 구글이 노린 걸까요?



<img src="https://k.kakaocdn.net/dn/c4zKuN/btrmEFuAMzb/m0jkijFnj1yEyruuXugro0/img.png" alt="img" style="zoom:67%;" />

재미없지만 재미있는 BERT (이미지 출처: Codemotion Magazine)

 **1)** 트랜스포머 설명과 BERT의 구조

 먼저 BERT*가 무엇의 약자인지부터 살펴보겠습니다. BERT는 Bidirectional Encoder Representations from Transformers의 약자입니다. 한국어로 번역하면 트랜스포머스를 사용한 양방향 인코더라고 할 수 있겠네요. 트랜스포머에 대해 간단히 설명하고 넘어가자면 입력된 문장의 토큰마다 Q(Query), K(Key), V(Value) 값을 생성하고, 이 값을 바탕으로 어떤 토큰이 얼마나 중요한지, 즉, 얼마나 해당 토큰에 집중(attention)해야하는지 점수(score)를 매깁니다. 이 점수를 매기는 방식을 병렬적으로 처리합니다. 이를 멀티 헤드 어텐션이라고 하며, 순서를 가지는 RNN 계열 딥러닝 모델이 아닙니다. BERT는 이런 트랜스포머의 인코더 부분이 여러 겹으로 쌓여 있는 구조입니다. 레이어 내부에서 토큰들이 모든 다른 토큰들을 참고합니다. 물론 똑같은 정도로 참고하는 것이 아닌 앞서 언급한 attention score에 따라 다릅니다. 모델의 구조도 장점이지만 BERT의 가장 큰 장점은 사전 훈련(pre-trained) 되어 있다는 것입니다.

 

 **2)** BERT의 사전 훈련(pre-training)

 보통 인공지능 모델을 만든다면 학습을 모델 설계자가 목적에 맞게 처음부터 훈련시켜야합니다. 하지만 BERT는 구글이 25억 단어의 위키피디아와 8억 단어의 BookCorpus, 총 33억 단어로 이미 학습한 상태입니다. 4일에 걸쳐 훈련되었다고 하는데 시간 뿐 아니라 어느 정도의 전력을 사용했을지도 궁금해지네요! 이 사전 학습은 크게 마스크드 언어 모델과 다음 문장 예측, 두 가지로 이루어집니다.

 마스크란 어떤 단어나 토큰을 가리는 것을 의미합니다. BERT는 먼저 무작위로 15%의 단어들을 선정합니다. 다음으로 선정된 단어들을 마스킹하거나, 변경하거나, 그대로 둡니다. (대부분 마스킹 합니다.) 마스킹되거나 변경된 단어들을 맞추는 방식으로 학습이 진행됩니다. 이렇게 단어를 맞추는 과정에서 다른 단어들을 참고하기 때문에 양방향성을 가집니다. 또한 label이 없기 때문에 비지도학습이라고 할 수 있겠습니다.

 다음 문장 예측의 경우, 실제 이어진 문장 반과 무작위로 이어붙인 두 개의 문장으로 훈련을 하는 방식입니다. 이어지는 문장인지 아닌지의 label이 존재하므로 이는 지도학습이라고 할 수 있겠습니다.

 

 **3)** BERT의 파인 튜닝(fine-tuning)

 이렇게 훈련되어 있으면 우리는 감사해하며 사용해야겠죠? 파인 튜닝이란 사용자가 해결하고자 하는 문제에 대해 출력층을 추가한 뒤에 데이터를 추가로 학습 시키는 것입니다. 파인 튜닝 과정에서는 추가 학습을 하는 동안 모델의 전체 파라미터(가중치나 편향 등)가 변화합니다. 즉, 이미 학습된 모델에 대해 사용자가 원하는 태스크에 맞춰 모델을 '미세 조정'한다고 이해할 수 있습니다. BERT의 fine-tuning으로 해결하고자 하는 과제는 크게 다음과 같은 4가지가 있습니다.

 

①텍스트 하나에 대한 분류

②토큰의 태깅

③텍스트 쌍 분류/회귀

④질문과 대답

 

Team.AK가 하려는 악플을 분류하는 태스크의 경우 텍스트를 입력으로 넣으면 해당 텍스트가 악플인지 아닌지를 출력으로 나타내는 문제이므로 ①텍스트 하나에 대한 분류 문제라고 할 수 있습니다. 따라서 데이터 불균형 문제만 처리할 수 있다면 설명한 바와 같이 자연어처리에서 강력한 BERT를 통해 잘 해결할 수 있겠습니다.

## 3. Methodology: 그걸 어떻게 할 건데?

 그럼 이제 프로젝트를 어떻게 진행할지 설명하겠습니다. 먼저 클래스 불균형 데이터를 선정합니다. 해당 프로젝트는 이진 분류로 문제를 해결해 볼 생각입니다. 그렇다면 이진 분류 문제 중, 클래스가 9:1에서 8:2의 비율을 가지고 있는 데이터 셋을 구해야 합니다. 이렇게 클래스 불균형 데이터를 선정했다면 알고 있거나 조사한 불균형 데이터 해결 방법 중 어떤 방법으로 클래스 불균형 문제를 해결할 것인지 선정합니다. 이렇게 선정 된 방법을 바로 불균형 데이터에 적용하면 적용하기 전과 비교할 수 없겠죠? 먼저, 불균형 미처리 데이터를 fine-tuning할 BERT모델 하나와, 처리된 데이터로 fine-tuning할 BERT모델, 이렇게 두 모델을 불러옵니다. 다음으로 처리되지 않은 클래스 불균형 데이터를 가지고 BERT를 fine-tuning합니다. 그 이후에 불균형 데이터를 처리하고 처리한 데이터를 가지고 fine-tuning합니다. 마지막으로 같은 test set으로 둘의 loss와 accuracy를 비교합니다.

 정리하면 다음과 같습니다.

