# 딥러닝

## 1. 인공지능이란?

1. 인간처럼 생각하는 시스템
2. __인간처럼 행동하는 시스템__
3. 이성적으로 생각하는 시스템
4. 이성적으로 행동하는 시스템



### 1. 앨런튜링

- 튜링머신을 탄생(암호해석 기계)



### 2. 인공지능 뇌의 진화

- 진공관 -> 트랜지스터 - > 직접회로->pc-> 분산 시스템



### 3. 최초의 인공 신경망

웨렌 맥컬록, 윌터 피츠 -> 인간의 뇌에서 뉴런 모델링

로젠 블렛 : 인간의 신경망 구조를 수학적, 공학적으로 모델링

우리는 여기서 가중치(w)를 찾는 weight patter을 분석한다.



## 2. 기계 학습(머신러닝)

- __데이터를 컴퓨터에 학습시켜 그 패턴과 규칙을 컴퓨터가 스스로 학습하도록 만드는 기술__
- 경험을 통해 나중 유사하거나 같은일을 효율적으로 처리할 수 있도록 시스템의 구조나 파라미터를 바꾸는것
- 이전에는 사람이 지식을 직접 데이터베이스화한 후 컴퓨터가 처리하도록 프로그래밍하였다.
- 데이터를 분류하는 수학적 모델을 프로그래밍, 데이터 입력시 이미 만들어진 수학적 모델이 규칙으로 적용되어 여러 문제를 풀 수 있음
- 기계가 패턴을 학습하여 자동화 하는 알고리즘
- 데이터를 입력해 규칙을 얻는다.

|          경험           |           일           |  효율  |
| :---------------------: | :--------------------: | :----: |
|          문자           |       문자 판독        | 정확도 |
| 사진(이미지), 얼굴 인식 | 사진에서 얼굴영역 식별 | 정확도 |
|    이메일, 스팸영역     |    스팸이메일 판단     | 정확도 |
|        풍경사진         |  유사한 풍경사진 식별  | 유사도 |
|        바둑 대결        |     바둑 두는 방법     |  승률  |



### 1. 모델(함수)

- 상관관계를 식으로 표현 : y = ax+b
- 가장 적합한 방정식을 완성한다면 새로운 입력에대한 출력값을 예측(prediction) 가능



### 2. 데이터의 구분

- 학습데이터
  - 필기문자 인식  :  MINIST(필기숫자데이터베이스)
  - 패턴또는 모델(함수)를 추출하기 위해서 사용하는 함수
- 테스트데이터
  - 학습된 모델의 성능을 평가하는데 사용하는데이터
  - 학습에는 사용되지 않았어야함
- 검증 데이터
  - 학습 과정에서 학습을 중단할 시점을 결정하기 위해 사용하는 데이터 집합



### 3. 학습 방식

- 연역적 학습
  - 연역적 추론을 통한 학습 / 두괄식 / (보고)
- 귀납적 학습
  - 미괄식, 과학적 탐구 // 논문에 쓰는 방식
  - 사례들을 일반화하여 패턴 또는 모델을 추출
  - 일반적 기계학습의 대상
  - 오킴의 면도날 : 원리에 따른 선택 / 가급적 학습결과를 간단한 형태로 표현하는 것이 좋다.



## 3. 기계학습의 종류

- 지도학습
  - 문제와 답을 함게 학습
  - 입력-출력을의 데이터들로부터 새로운 입력에 대한 출력을 결정할 패턴을 추출
- 비지도 학습
  - 조력자의 도움없이 컴퓨터가 스스로 학습.
  - 컴퓨터가 훈련 데이터(레이블x)를 이용하여 데이터들 간의  규칙성(패턴)을 찾아냄
  - 출력정보가 없는 훈련데이터로 입력데이터들의 패턴을 학습
- 반지도 학습
  - 일부 학습 데이터만 출력값이 주어진 상태에서 일반화 패턴 추출
- 강화 학습
  - 출력에대한 정확한 정보를 제공하지 않지만, 평가정보(reward)를 주어지는 문제에 대해 각 상태에서의 행동을 결정
  - 컴퓨터가 세상에 존재하는 규칙을 스스로 시뮬레이션하면서 게임처럼 규칙을 학습



## 4. 지도학습

- 주어진 (입력, 출력)데이터 를 이용하여 패턴을 추출
- 새로운 입력이 있을 때 결과를 결정할 수 있는 방법을 찾아낸는 것
- 분류
  - __출력이 정해진 부류중(class, category) 중의 하나로 결정__
  - 이산형 값인 y의 특징을 찾고 데이터 x를 사용하여 y값을 예측하는 기법
  - 분류 문제의 학습
    - 학습 데이터를 잘 분류할 수 있는 함수를 찾는 것
    - 분류기(classfier)
      - 수학적 함수 또는 규칙일 수 도 있다.
      - 학습된 함수를 이용하여 데이터를 분류하는 프로그램
      - 이상적인 분류기 
        - 학습에 사용되지 않은(테스트 데이터)에 대해서 분류를 잘 하는 것
        - 일반화 능률이 좋은것
- 회귀 분석
  - __출력이 연속인 영역의 값을 결정__
  - 연속형 값인 y의 특징을 찾고 데이터 x를 사용하여 y값을 예측하는 기법



## 5. 분류

### 1. 과적합과 부적합

- 과적합

  - __학습데이터에 대해서 지나치게 잘 학습된 상태__

  - 분류기가 꼬불꼬불한 모형

  - 오류나 잡음 포함된 학습데이터에 대해 매우 높은 성능을 보이더라도 학습되지 않은 데이터(테스트데이터)에 대해 좋지 않은 성능을 보일 수 있음

  - __회피방법__

    - 학습데이터에 대한 성능

      - 학습을 진행할 수록 오류 개선 경향
      - __지나치게 학습이 진행되면 과적합 발생__

    - 학습과정에서 별도의 검증 데이터에 대한 성능 평가

      - 검증데이터에 대한 오류가 감소하다가 증가하는 시점에 학습 중단

      

- 부적합(undefitting)

  - __학습 데이터를 충분히 학습하지 않은 상태__
  
  

### 2. 분류기

- 분류기의 성능평가
  - 정확도 : 95% 이상 되게
    - 정확도 = (옳게 분류한 데이터 개수) / (전체 데이터 개수)
    -  __테스트 데이터에 대한 정확도를 분류기의 정화도로 사용__
  - 정확도가 높은 분류기를 학습하기 위해서는 __많은 학습데이터__를 사용하는 것이 유리
  - 학습데이터와 테스트데이터는 겹치게 않도록 해야 함
  - 오류율 = 잘못 분류한 데이터수 / 전제 데이터수 = 1-정확도



- __학습 데이터가 부족한 경우 성능 평가__

  - 별도로 테스트 데이터를 확보하면 비효율적

    - 인위적으로 데이터 만들기 : RST

  - 가능하면 많은 데이터를 학습에 사용하면서, 성능 평가하는 방법 필요

  - k-겹 교차검증 사용

    - 전체 데이터를 k등분
    - 각 등분을 한번씩 테스트 데이터로 사용하여, 성능평가를 하고 평균값 선택

    

- **불균형 데이터 문제**

  - 특정 부류에 속하는 학습 데이터의 개수가 다른 부류에 비하여 지나치게 많은 경우
  
  - __학습데이터의 부류 분포가 고르지 못해 생기는 현상__
  
  - 정확도에 의한 성능평가가 무의미할 수 있음
  
  - 문제해결
    - 인공 학습 데이터 생성
    
    - SMOTE 알고리즘(인공 소수 샘플링 알고리즘)
    
      - 빈도가 낮은 부류의 학습 데이터를 인공적으로 만들어 내는 방법
    
      1. 임의로 낮은 빈도 부류이 학습 데이터 x선택
      2. x의 k-근접이웃(KNN)인 같은 부류의 데이터 선택
      3. k-근접이웃 중에 무작위 하나 y를 선택
      4. x와 y를 연결하는 직선상의 무작위 위치에 새로운 데이터 z생성



- 이진 분류기의 성능평가

  - 이진 분류기 : 로지스틱 회귀 모델

    - 두개의 부류만을 갖는 데이터에 대한 분류기 (0이나 1/ positive이나 negative)

    |                  |            |           예측(출력)           |                                |
    | :--------------: | :--------: | :----------------------------: | :----------------------------: |
    |                  | 데이터종류 |              양성              |              음성              |
    |       실제       |    양성    | 진양성(True Positive)<br />TP  | 위음성(False Negative)<br />FN |
    | 실제데이터(입력) |    음성    | 위양성(False Positive)<br />FP | 진음성(True Negative)<br />TN  |

    - 이진분류기의 성능 평가 척도 : 민감도
    - 민감도 = TP / (TP+FN) : 실제 데이터가 양성일 떄 분류기가 양성으로 판정한 것의 비율
    - 위양성률 = 실제 음성인것 중에 분류기가 양성으로 판정. 1-특이도
    - 특이도 = TN/FP+TN : 실제 데이터가 음성일 때 분류기가 음성으로 판정한 것의 비율
    - __정밀도  =  TP / (TP+FP) : 실제 양성 비율__
    - __정확도  =  TP+TN / TP+FP+TN+FN : 전체중에서 옳게 분류한 것의 비율__
    - ROC 곡선
      - 부류 판정 임계값에 위양성률, 민감도  그래프
      - 민감도 값이 클수록 좋고 위 양성률이 작을수록 좋은 분류기이다. (이상분류 위=0, 민간도 =1)
    - AUC
      - ROC 곡선에서 곡선 아래 부분의 면적
      - 클 수록 바람직. 양호

## 6. 회귀

- 학습데이터(x)에 부합되는 출력값이 실수인 함수를 찾는 문제
- 근사함수 이용 : 중고자동차 판매(주행거리에 따라 가격 결정)
- 목적함수를 찾는 것
  - 에러값의 합이 가장 작은 함수가 회귀함수(분산이 가장 낮은 함수)
  - 학습데이터(xi,yi)
- 성능
  - 목적함수 :  오차 : 예측값과 실제값의 차이 J(e)
  - 테스크 데이터들에 대한 분산 또는 표준편차 
  - MQE(mean square error)
  - 모델의 종류(함수의 종류)에 영향을 받음
- 회귀의 과적합과 부적합
  - 과적합
    - 지나치게 복잡한 모델 사용
    - 대응방법
      - 모델의 복잡도를 성능 평가에 반영
  - 부적합
    - 지나치게 단순한 모델 사용
- 로지스틱 회귀
  - 학습데이터 : {(x1,y1), (x2,y2), ..., }, 단 y의 치역은 {0,1} 인 입출력쌍 (지도학습)
  - 로지스틱 함수를 이용하여 함수 근사 : 출력0 혹은 1
  - 로지스틱 함수식 
  - 학습시 목적 함수 : 교차 엔트로피
  - 경사 하강법 사용 학습 : 목적함수 J(e)를 최소화 시키는 e를 찾음



## 7. 비지도 학습

- 비지도 학습
  - 결과정보가 없는 데이터들에 대해서 특정 패턴을 찾는 것
- 군집화
- 밀도추정
- 차원축소



### 1. 군집화

- 군집화
  - 유사성에 따라 데이터를 군집으로 나누는 것, 분할 하는 것,
  - 동일 군집은 데이터 비슷
- 일반 군집화
  - 하나의 데이터는 하나의 군집에만 소속
  - k-means 알고리즘
- 퍼지 군집화
  - 데이터가 여러 군집에 부분적으로 소속
  -  소속정도의 합은 1이 됨
  - 퍼지 k-means 알고리즘
- 성능
  - 군집내의 분산과 군집간의 거리



### 2 밀도추정

- 밀도추정
  - 부류별 확률적 특성 이용한 분류 방법
  - 부류별 데이터를 만들어 냈을 것으로 추정되는 확률분포를 찾는 것
- 용도
  - 각 부류 별로 주어진 데이터를 발생시키는 확률 계산
- 모수적 밀도 추정 : 가우시안 모델(평균과 분산을 사용)

- 비모수적 밀도 추정 : 히스토그램 추정, 커널밀도 추정 모델



### 3 차원 축소

- 고차원의 데이터를 정보의 손실을 최소화하면서 저차원으로 변환하는 것
- 목적
  - 2, 3차원으로 변환해 시각화하면 직관적 데이터 분석 가능
  - 차원의 저주
    - 차원이 커질수록 데이터간 거리가 좁아짐 : 거리분포가 일정해지는 경향
    - 차원이 증가함에 따라 부분공간의 개수가 기하급수적으로 증가

- 주성분 분석(PCA)
  - 분산이 큰 소수의 축들을 기준으로 데이터를 사상하여 저차원으로 변환
  - 데이터의 공분산행렬에 대한 고유값이 큰 소수의 고유벡터를 사상 축으로 선택




### 4 이상치 탐지

- 이상치
  - 다른 데이터와 크게 달라서 다른 메커니즘에 의해 생성된 것이 아닌지 의심스러운 데이터
  - **관심대상**
- 잡음
  - 관측오류
  - **제거대상**



## 8. 결정트리를 이용한 분류

- 학습데이터로부터 규치형태의 지식을 추출하는 방법
- 트리형태로 의사결정 지식을 표현한 것
  - 내부노드 : 비교속성, 판단을 위한 조건
  - 간선 : 속성 값
  - 단말 노드 : 부류 정보, 대표값

- 결정트리 알고리즘
  - 모든 데이터를 포함한 하나의 노드로 구성된 트리에서 시작
  - 반복적인 노드 분할 과정
    - 분할 속성을 선택 : A
    - 속성값에 따라 서브트리를 생성(결정)
    - 데이터를 속성값에따라 분배
  - 분할 속성 결정
    - 어떤  속성을 선택하는 것이 효율적인가
      - __분할한 결과가 가능하면 동질적인 것으로 만드는 속성선택__
      - 분할된 각 데이터들이 동일한 부류되게 속성 선택
    - 엔트로피
      - 동질적인 정도 측정 가능 척도
      - I = 시그마(-PlogP)  / P는 부류 c에 속하는 것의 비율
      - 각 부류의 데이터들이 균등한 비율로 섞여 있을수록 엔트로피가 크다.
      - __골고루 섞인 정도가 클 수록 큰 값__
    - 정보 이득
      - IG = I-Ires
      - I : 전체 데이터 엔트로피
      - Ires : 특정속성으로 분할한 후의 각 부분집합의 정보량의 가중평균
      - __정보이득(IG)이 클수록 우수한 분할 속성__
      - __결정 트리 알고리즘은 정보이득이 가장 큰 속성을 분할속성으로 사용__
      - 속성값이 많은 것 선호
      - 속성값이 많으면 데이터집합을 많은 부분집합으로 분할
        - 작은 부분집합은 동질적인 경향
      - 개선척도
        - 정보이득비
          - 속성값이 많은 속성에대해 불이익
          - GainRatio(A) = IG(A)/I(A) = I-Ires(A)/I(A)
          - I(A)
            - 속성 A의 속성값(pattern)을 부류로 간주하여 계산한 엔트로피
        - 지니지수
          - 데이터 집합에 대한 지니 값
          - 속성 A에 대한 지니 지수값 가중평균
          - 지니 지수 이득 : GiniGain
    - ID3 알고리즘
      - 범주형 속성값을 갖는 데이터에 대한 결정트리 학습 EX)  삼각형/사각형



## 9. 결정트리를 이용한 회귀

- **출력값이 수치값**
- 분류를 위한 결정트리와 차이점
  - 단말노드가 부류가 아닌 수치값임
  - 해당 조건을 만족하는 것들이 가지는 대표값
- 분할 속성 선택
  - __표준편차 축소 SDR__를 최대로 하는 속성 선택
    - SDR(A) = SD-SD(A)
    - SD : 전체 표준편차
    - SD(A) : 속성 A를 기준으로 분할 후의 부분 집합별 표준편차의 가중평균
      - SD(Pattern) = 패턴으로 분할 후 의 부분집합별 표준편차의 가중평균



## 10. K-근접이웃(KNN) 알고리즘

- (입력,결과)가 있는 데이터들이 주어진 상황에서 새로운 입력에대한 결과를 추정할 때 결과를 아는 최근접한 k개의 데이터에 대한 결과 정보를 이용하는 방법
- target에서 가장 근접한 데이터들
- 질의(입력)와 데이터간의 거리 계산
  - 수치데이터의 경우 : 두 점 사이의 거리
- 효율적으로 근접이수 탐색
- 근접 이웃 k개로부터 결과를 추정
- 효율적인 근접 이웃탐색
  - 데이터의 개수가 많아지면 계산시간 증가
- 최근접 k개로부터 결과를 추정하는 방법
  - 분류
    - 출력이 범주형 값
    - 다수결 투표  : 개수가 많은 범주 선택
  - 회귀
    - 출력이 수치형값
    - 평균 : 최근접 k개의 평균값
    - 가중합 : 거리에 반비례하는 가중치 사용
- 특징
  - 학습단계에서는 실질적인 학습X. 데이터만 저장
    - 학습데이터가 크면 메모리문제
    - 게으른 학습
  - 저장된 데이터 이용 학습시 시간이 많이 걸릴 수 있음



## 11. 군집화 알고리즘

- 데이터를 유사한 것들끼리 모우는 것

- __군집 간의 유사도는 크게 , 군집내의 유사도는 작게__

- 계층적 군집화
  - 군집화의 결과가 군집들이 계층적인 구조를 갖도록 하는 것
  - 병합형 계층적 군집화
    - 각 데이터가 하나의 군집을 구성하는 상태에서 시작하여, 가까이에 있는 군집들을 결합하는 과정반복하여 계층적인 군집 형성
  - 분리형 계층적 군집화
    - 모든 데이터를 포함한 군집에서 시작하여 유사성을 바탕으로  군집을 분리하여 점차 계층적인 구조를 갖도록 구성
  
- 분할 군집화
  - 계측정 구조를 만들지 않고 전체 데이터를 유사한것 끼리 나누어 묶는 것
    - ex) k-means 알고리즘
  
- 계층적 군집화와 덴드로그램
  - 덴드로그램 : 
    - 높이는 군집내의 데이터간의 거리
    - 가지는 대응하는 군집들이 결합되는것
    
    

## 11-1. **K-means알고리즘**

- 군집화 알고리즘
- 군집화 과정
  1. 무작위로 k개를 선택하여 초기 군집의 중심위 선정
  2. 군집 중심을 기준으로 각 데이터와 중심간 거리 계산 : 군집 재구성
  3. 각 데이터를 가장 가까운 군집으로 배정 : 군집별 평균위치 결정
  4. 각 군집에 대해 배정된 모든 데이터 평군 계산하여 새로운 군집 중심 정함 : 군집 평균 위치로 군집 중심 조정
  5. 수렴할 떄 까지 2-4과정 반복

- 전체 분산
  - 각 클로스터의 분산을 다 더한 v
  - 분산값 v를 최소화하는 Si를 찾는 것이 알고리즘의 목표

- 특성
  - 군집의 개수 k는 미리 지정
  - 초기 군집 위치에 민감



## 11-2. **단순 베이즈 분류기**

- 부류(class) 결정지식을 조건부 확률로 결정
- P(c|x1,x2,...,xn) : 속성값이 부류에 포함될 조건부 확률
- 가능도의 조건부 독립 가정
  - P(c|x1,x2,...,xn) = P(x1|c)P(x2|c)...P(xn|c) / P(x1,x2,...,xn)

