
[toc]
# 토큰화

- 단어 토큰화(Word Tokenization)
- 문장 토큰화(Sentence Tokenization)



## 1. 단어 토큰화(Word Tokenization)

- 토큰 기준을 단어로 하여 토큰화 하는것
- 단어(word)는 단어 단위 외에도 단어구, 의미를 갖는 문자열로도 간주됨
- 보통 토큰화 작업은 단순한 구두점이나 특수문자를 전부 제거한 정제(cleaning)작업을 수행하는 것만으로 해결되지 않음
- 구두점이나 특수문자를 전부 제거하면 어떤 토큰은 의미를 잃어 버리는 경우가 발생함
- 띄어쓰기 단위로 자를시 단어 토큰 구분이 망가지는 언어도 존재
  - ex) 데이터 과학
- 토큰화 진행 시, 예상하지 못한 경우가 있어서 토큰화의 기준을 생각해봐야 하는 경우가 발생함 Ex) 영어의 어퍼스트로피 토큰화 문제 Don't

|           코드 및 모듈           |                             기능                             |
| :------------------------------: | :----------------------------------------------------------: |
|  nltk(Natural Language Toolkit)  |          자연어(영어) 처리 및 문자열 분석용 패키지           |
| konlpy(Korean Natural Language ) |             한국어 자연어 처리 및 분석용  패키지             |
|  kss(Korean Sentence Splitter)   | 한국어 문장 구분기 제공 패키지(**New Line(\n)을 포함한다.**) |

```python
from nltk.tokenize import word_tokenize
# 가장 많이 사용하는 자연어(영어) 단어 토큰화기. 형태소 기준 으로 분류

from nltk.tokenize import WordPinctTokenizer
#또 다른 단어 토큰화기 패키기를 호출. 단어 기준으로 분류

from tensorflow.keras.preprocessing.text import text_to_word_sequence
#keras의 텍스트 전처리 패키지에서 토큰화된 text를 sequence(벡터)로 ㅂ

TreebankWordTokenizer().tokenize(text)
#nktl의 또 다른 단어 토큰화기 = word_tokenize()와 동일하다.

from nltk.tag import pos_tag #pos(part of sentence = 품사)
t_t = word_tokenize(t6) #단어 토큰화
pos_tag(t_t) #토큰화된 리스트의 각 원소에 품사 태그 붙인다.

from tensorflow.keras.preprocessing.text import Tokenizer
#문장으로부터 단어를 토큰화하고 숫자에 대응시키는 딕셔너리를 사용할 수 있도록 합니다.
tokenizer= Tokenizer(num_words=100) #빈도수 기준 상위 100-1개 단어 토큰만이 고려
#fit_on_texts() 메서드는 문자 데이터를 입력받아서 리스트의 형태로 변환합니다.
#토큰화 인자는 문서이다.
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index
print(word_index)
sequences = tokenizer.texts_to_sequences(sentences)
#텍스트를 시퀀스의 형태로 변경
#이 리스트는 빈도수 정수 인덱스
# 문서를 정수 인코딩하는거다.



```



- 토큰화 고려 사항 
  - 구두점이나 특수 문자를 단순 제외해서는 안 된다.
    - 구두점조차 하나의 토큰으로 분류하기도 한다.
    - 마침표(.)
      - 문장의 경계를 통한 단어를 추출용 기준 이용
    - 단어 자체 특수문자나 구두점
      - m.p.h Ph.D나 AT&T 특수 문자의 달러나($)나 슬래시(/)
    - 숫자 사이에 컴마(,)
      - 123,456,789
  - 줄임말과 단어 내에 띄어쓰기가 있는경우 
    - what're 는 what are의 줄임말
    - we're는 we are의 줄임말
    - New york은 단어 자체에 띄어쓰기가 존재
- Penn Treebank Tokenization의 규칙
  - 영어 토큰화 표준으로 사용되는 규칙(=word_tokenization)
  - 규칙1 : 하이푼으로 구성된 단어는 하나로 유지
  - 규칙2 : doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리



## 2. 문장 토큰화(Sentence Tokenization)

- 문장 단위로 구분하는 작업으로 때로는 문장 분류

- 정제되지 않은 상태라면, 코퍼스는 문장 단위로 구분되어 있지 않아서 이를 사용하고 하는 용도에 맞게 문장 토큰화가 필요.

- !나 ?는 문장의 구분을 위한 꽤 명확한 구분자(boundary) 역할

- 마침표. 는 문장의 끝이 아니더라도 등장할 수 있음=> 단어 자체의 . 이 들어갈 수 있다.

  - EX1) IP 192.168.56.31 서버에 들어가서 aaa@gmail.com로 굙허 좀 보냐줘
  - EX2) Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.

- ```python
  #문장 토큰화기(영어)
  from nltk.tokenize import sent_tokenize
  sent_tokenize(t3)
  
  #문장 토큰화기(한글)
  import kss #Korean Sentence Spliter
  kss.split_sentences(text
  ```



## 3. 한국어 토큰화

- 영어는 New York과 같은 합성어나 he's와 같이 줄임말에 대한 예외처리만 한다면, 띄어쓰기 기준으로 띄어쓰기 토큰화를 수행해도 단어토큰화가 잘 작동
- 한국어는 영어와 달리띄어 쓰기만으로는 토큰화를 하기에 부족
- 한귝어의 경우 띄어쓰기 단위가 되는 단위를 어절이라고 하는대 어절 토큰화는 한국어 NLP에서 지양되고 있음
- 어절토큰화와 단어토큰화는 다름
- 한국어가 영어랑 다른 형태를 가지는 언어인 교착어라는 점에서기인
- 교착어란 조사, 어미 등을 붙여서 말을 만드는 언어를 말함
  - 조사가 존재
    - 그라는 단어 하나에도 '그가', '그에게', '그를' ,'그와', '그는'과 같이 다양한 조사가 '그'라는 글자 뒤에 띄어쓰기 없이 바로 붙게됨
    - 대부분의 한국어 NLP에서 조사는 분리해줄 필요가 있음
  - 띄어쓰기 단이가 영어처럼 독립적인 단어가 아님
  - 한국어는 어절이 독립적인 단어로 구성되는 것이 아니라 조사 등의 무언가가 붙어있는 경우
    가 많아서 __이를 전부 분리해줘야 함__

## 4. 형태소

- 한국어 토큰화에서는 형태소란 개념을 반드시 이해해야 함
- 형태소
  - 뜻을 가진 가장 작은 말의 단위
  - 이 형태소에는 두가지 형태소가 있는데 자립과 의존
  - 한국어의 문장=> 어절(띄어쓰기 단위)로 구성 =>  어절은 다양한 형태소로 구상
- 자립 형태소
  - 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소
  - __그 자체로 단어가 됨__
  - 체언, 수식언, 감탄사
- 의존 형태소
  - 다른 형태소와 결합하여 사용되는 형태소
  - 접사, 조사, 어미, 어간
- 한국어에서 영어에서의 단어 토큰화와 유사한 형태를 얻어내려면 어절토큰화가 아난 형태소 토큰화를 수행해야 한다.

```python
from konlpy.tag import Okt #세종 품사 사전. 오픈 코리언 태그 형태소
from konlpy.tag import Kkma #서울대 제작 사전

nl = Okt() #형태소 토큰화기 객체
n1.morphs(text) #텍스트에서 형태소 반환한다.
n1.pos(text) #텍스트에서 품사 정보를 부착하여 반환한다.
#영어 단어 토큰화에서는 pos_tag(토큰화된 리스트)

n1.nouns(text)

n2=Kkma() #어절,조사,접사 모두 형태소로 분리
n2.morphs(t7),n2.pos(t7),n2.nouns(t7)

```

- 품사 태깅(part-of-speech tagging) – 단어 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지를 구분해놓는 작업