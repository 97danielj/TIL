[toc]

# 토큰화, 정수인코딩 정리

## 토큰화

### 1. 영어

#### 1. 단어 토큰화

1. word_tokenize() 
2. wordPunctTokenizer()
3. 케라스의 Text_to_word_seqeunce()

#### 2. 문장토큰화

1. sentence_tokenizer

### 2. 한글

#### 1. 형태소 토큰화

1. okt().morphs()

#### 2. 문장 토큰화

2. kss.sentence_split



## 정수 인코딩

- **단어에 고유 정수 인덱스를 부여하는 것. 단어 집합을 생성하는 것**
- **입력데이터는 토큰화된 문서**
- word_index에 있는 부여된 인덱스 기반으로 텍스트를 정수로 변환

- **보통은 단어 등장 빈도수를 기준으로 정렬한 뒤에 부여합니다.**

1. 케라스의 텍스트 전처리 

2. ```python
   from tensorflow.keras.preprocessiong.text import Tokenizer
   tokenizer = Tokenizer()
   # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성.
   # 단어 집합은 : word_index로 확인 가능
   tokenizer.fit_on_texts(preprocessed_sentences)
   print(tokenizer.word_counts)
   #빈도수 기반으로 단어 나열
   print(tokenizer.texts_to_sequences(preprocessed_sentences)) #텍스트를 시퀀스로 변환
   tokenizer.word_index #단어 집합 확인
   ```

3. 케라스 토크나이저는 기본적으로 단어 집합에 없는 단어인 OOV에 대하서는 아예 단어를 제거한다는 특징이 있다. 단어 집합에 없는 단어들을 OOV로 간주하여 보존하고 싶다면 Tokenizer의 인자 oov_token을 사용합니다.

4. 또한 단어집합에 숫자 0에 지정된 단어가 존재하지 않는데도 케라스 토크나이저가 숫자 0까지 단어집합 산정 이유는 자연어 처리에서 패딩이라는 작업때문입니다.

5. ```python
   # 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2
   vocav_size = 5
   okenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')
   tokenizer.fit_on_texts(preprocessed_sentences)
   ```



## 원- 핫 인코딩

**단어 집합(vocabulary)** : 서로 다른 단어들의 집합. 중복은 허용치 않는다.

Tokenizer에서는 word_index가 유사하다.

원핫인코딩 : 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 벡터 표현 방식

**i) 정수 인코딩을 수행한다. 각 단어에 고유한 정수를 부여한다.**

**ii) 표현하고 싶은 단어의 고유한 정수를 인덱스로 간주하고 해당 위치에 1을 부여하고, 다른 단어의 인덱스의 위치에는 0을 부여한다.**

```python
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
#정수 인코딩 부터 시작한다.
tokenizer = Tokenizer()
tokenizer.fit_on_texts([text])
print('단어 집합 :',tokenizer.word_index)
sub_text = "점심 먹으러 갈래 메뉴는 햄버거 최고야"
encoded = tokenizer.texts_to_sequences([sub_text])[0]
print(encoded)
#인코딩된 시퀀스를 to_categorical()함수에 전달
one_hot = to_categorical(encoded)
print(one_hot)
```









## BOW(빈도 수 기반 택스트 데이터의 수치화 표현)

1. BOW : **단어의 등장 순서를 고려하지 않는 빈도수 기반의 텍스트 표현 방법**

2. 각 단어가 등장한 횟수를 수치화 하는 

   1. CounterVectorizer 
   2. CounterVecotrizer과 Tokenizer의 차이점
      1. CounterVectorizer는 낮은 수준의 토큰화도 동시에 해준다.

3. ```python
   from sklearn.feature_extraction.text import CountVector
   corpus = ['you know I want your love. because I love you.']
   vector = CountVectorizer()
   # 코퍼스로부터 단어사전 생성, 각 단어의 빈도수를 기록
   print('bag of words vector :', vector.fit_transform(corpus).toarray())
   # 각 단어의 인덱스가 어떻게 부여되었는지를 출력
   print('vocabulary :',vector.vocabulary_)

4. 각 문서벡터의 차원이 단어집합의 크기가 된다.

## DTM 

- 로 문서들로 이루어진 행렬을 (BOW)카운트기반으로 변환
- 특징
  - 희소표현
  - 단순 빈도 수 기반 접근
    - DTM에 불용어와 중요한 단어에 대해서 가중치를 줄이는 방식이 필요

## TF-IDF

- DTM을 만든 후, TF-IDF 가중치를 부여합니다.
- 여러 문서 내에서 등장한 단어는 IDF값이 낮지만 한 문서 내에서 여러번 반복해서 출현했더라면 TF값은 상승하니 문서 내 중요 단어는 TF-IDF 값은 올라간다.
- 순서
  - CounterVectorizer로 코서프를 BOW벡터화(카운트기반 문서표현)
  - TfidfVectorizer 로 카운터벡터가 변환시킨 문서 행렬을 변환
