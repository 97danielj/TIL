[toc]

# 02) GPT - 2

- Language Models are Unsupervised Multitask Learners
- 버전 발전 과정
  - ELMO -> GPT -> BERT-BASE -> GPT-2 -> GPT-3
- 기존 GPT랑 다를 게 없는 오직 Decoder's transformer을 사용
- 학습 된 데이터 셋만이 커졌다.

## 1. Transformer vs GPT vs BERT

- Transformer revisited
<img src="02.GPT-2.assets/image-20230222175922947.png" alt="image-20230222175922947" style="zoom:40%; float:left" /> <img src="02.GPT-2.assets/image-20230222175849394.png" alt="image-20230222175849394" style="zoom:40%; float : right" />
- 













![image-20230222180045087](02.GPT-2.assets/image-20230222180045087.png)



![image-20230222180119199](02.GPT-2.assets/image-20230222180119199.png)

- **Main difference between GPT-2 and BERT**
- GPT-2 is auto-regressive but BERT is not
  - After each token is produced, that token is added to the sequence of input

<img src="02.GPT-2.assets/image-20230222180458477.png" alt="image-20230222180458477" style="zoom:50%;" />



## 2. GPT 활용

GPT-2 can process 1024 tokens

- Each token flows through all the decoder blocks along its own path

<img src="02.GPT-2.assets/image-20230222180548825.png" alt="image-20230222180548825" style="zoom:50%;" />

### 1. **The simplest way to run a trained GPT-2 is to allow it to ramble on its own**

- Generating unconditional samples

- GPT-2 has a parameter called top-k that we can use to have the model consider 
  sampling words other than the top word

<img src="02.GPT-2.assets/image-20230222182045214.png" alt="image-20230222182045214" style="zoom:50%;" />

### 2. In the next step, we add the output from the first step to our input sequence, 
and have the model make its next prediction:

1. The second path is the only that’s active in this calculation
2. GPT-2 does not re-interpret the first token in light of the second token

<img src="02.GPT-2.assets/image-20230222182050824.png" alt="image-20230222182050824" style="zoom:50%;" />

- GPT2 : A deeper look inside
  - Sending a word to the first transformer block
  - <img src="02.GPT-2.assets/image-20230222181842273.png" alt="image-20230222181842273" style="zoom:50%;" />

### 3. A journey up the stack

- Once a lower-level transformer block processes the token, it sends its resulting vector up the stack to be processed by the next block

  - The process is identical in each block, but each block has its own weights in both selfattention and the neural network sublayers

  <img src="02.GPT-2.assets/image-20230222182308549.png" alt="image-20230222182308549" style="zoom:50%;" />

- Same Sturcuter, different weights for each DECODER



## 3. Self-Attention Recap

- Language heavily relies on context
- This self-attention layer in the top block is paying attention to “a robot” when it processes the word “it”

<img src="02.GPT-2.assets/image-20230222182808695.png" alt="image-20230222182808695" style="zoom:50%;" />

-  Think of it like searching through a filing cabinet
  - The query is like a sticky note with the topic you’re researching
  - <img src="02.GPT-2.assets/image-20230222184606161.png" alt="image-20230222184606161" style="zoom:50%;" />
  - The keys are like the labels of the folders inside the cabinet
  - When you match the tag with a sticky note, we take out the contents of that folder, these contents are the value vector
  - Except you’re not only looking for one value, but a blend of values from a blend of folders.

- Multiply each value by its score and sum up – resulting in our self-attention outcome
- <img src="02.GPT-2.assets/image-20230222191116318.png" alt="image-20230222191116318" style="zoom:50%;" />
  - This weighted blend of value vectors results in a vector that paid 50% of its attention to 
    the word robot, 30% to the word a, and 19% to the word it
-  Model Output
  - When the top block in the model produces its output vector (the result of its own 
    self-attention followed by its own neural network), the model multiplies that vector 
    by the embedding matrix
  - <img src="02.GPT-2.assets/image-20230222192021875.png" alt="image-20230222192021875" style="zoom:50%;" />
  - <img src="02.GPT-2.assets/image-20230222192051538.png" alt="image-20230222192051538" style="zoom:50%;" />

## 4. GPT-2 유의점

1. GPT-2 uses Byte Pair Encoding to create the tokens in its vocabulary; tokens are usually parts of words
2. When training, a maximum of 512 tokens are processes at the same time
3. Layer normalization is important in Transformer structure



## 5. GPT-2 성능

<img src="02.GPT-2.assets/image-20230222192939199.png" alt="image-20230222192939199" style="zoom:50%;" />
