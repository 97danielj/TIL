[TOC]

# K-최근접 이웃(KNN)

## 1. 개요

지금까지의 학습알고리즘과 근본걱으로 다른 `게으른 학습기(lazy learner)`입니다. 단순하기에 게으른 것이 아닌 알고리즘이 훈련 데이터에서 판별 함수를 학습하는 대신 훈련 데이터셋을 메모리에 저장하기 때문이빈다.

```tex
모수 모델 vs 비모수 모델
머신 러닝 알고리즘은 모수 모델과 비모수 모델로 묶을 수 있씁니다. 모수 모델은 새로운 데이터 포인트를 분류할 수 있는 함수를 학습하기 위해 훈련 데이터셋에서 모델 파라미터를 추정합니다.
훈련이 끝나면 원본 훈련 데이터셋은 더 이상 필요하지 않다. 전형적인 모수 모델은 퍼셉트론, 로지스틱 회귀, 선형 회귀, 선형 SVM입니다. 반대로 비모수 모델은 고정된 개수의 파라미터로 설명될수 없고, 훈련데이터가 늘어남에 따라 파라미터 개수도 늘어납니다.비모수 모델은 결정트리/랜덤 포레스트와 커널SVM이빈다.
KNN은 비모수모델에 속하며 인스턴스 기반 모델이라고 합니다. 인스턴스 기반 모델은 훈련 데이터셋을 메모리에 저장하는 것이 특징입니다.
```

**KNN알고리즘 단계**

1. 숫자 k와 거리 측정 기준을 선택합니다.

2. 분류하려는 샘플에서 k개의 최근접 이웃을 찾습니다.

3. 다수결 투표를 통해 클레스 레이블을 할당합니다.

4. ![img](https://blog.kakaocdn.net/dn/befOXs/btqBGZkO2lO/XSd2xMISWhSzMayd2fbgSk/img.png)

이러한 메모리 기반의 분류기는 수집된 새로운 데이터에 즉시 적응할 수 있는 것이 장점입니다. 단점은 새로운 샘플을 분류하는 계산이 상당히 복잡하다는 점이죠. 데이터의 차원이 적고 알고리즘이 효율적인 구조가 아니라면 계산의 복잡도는 샘플 개수에 선형적으로 증가할 겁니다. 또 훈련 단계가 없으니 훈련 샘플을 버릴 수 없어서 저장공간에 문제가 있을 수 있죠.

적절한 k를 선택하는 것이 오버피팅과 언더피팅 사이의 균형을 잡을 수 있습니다. 당연히 데이터의 특성에 알맞는 거리 측정 지표를 선택해야하죠. 붓꽃 데이터처럼 실수 값을 가진 경우에는 유클리디안 거리를 사용합니다. 유클리디안은 표준화해야한다는 것이 중요합니다. 위 코드에서 minkowski 거리는 유클리디안과 맨해튼 거리를 일반화한 것이다.

 매개변수 p=2로 지정하면 유클리디안이, p-1로 지정하면 맨해튼 거리가 됩니다. 

### 차원의 저주

KNN은 차원의 저주 때문에 과대적합되기 쉽다는 것이 중요하다. 차원의 저주는 고정된 크기의 훈련 데이터셋이 차원이 늘어남에 따라 특성 공간이 점점 희소해지는 현상입니다. 고차원 공간에서는 가장 가까운 이웃이라도 좋은 추정값을 만들기에는 너무 멀리 떨여져있다. 특성선택과 차원 축소 기법을 사용하면 차원의 저주를 피하는데 도움이 된다.
