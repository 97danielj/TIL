[toc]

# 5. 서포트 벡터 머신(SVM)을 사용한 최대 마진 분류

SVM은 퍼셉론의 확장이라고 생각할 수 있습니다. 퍼셉트론 알고리즘을 사용하여 분류오차를 최적화. **SVM의 최적화 대상은 마진을 최대화 하는 것입니다.**

결정경계는 무수히 많이 그을 수 있다. 최적의 결정경계는 무엇일까? **결정경계는 데이터군으로부터 최대한 멀리 떨어지는 게 좋다.** 실제로 서포트 벡터는 결정 걍계와 가까이있는 데이터 포인트들을 의미한다. 이 데이터들이 경계를 정의하는 결정적인 역할을 하는 셈이다.

마진(margin) : 결정 경계와 서포트 벡터 사이의 거리를 의미한다. **최적의 결정경계는 마진(거리)을 최대화 한다.**

![img](https://i0.wp.com/hleecaster.com/wp-content/uploads/2020/01/svm04.png?fit=1024%2C768)가운데 실선이 하나 그어져있는데, 이게 바로 ‘결정 경계’가 되겠다. 그리고 그 실선으로부터 검은 테두리가 있는 빨간점 1개, 파란점 2개까지 영역을 두고 점선을 그어놓았다. 점선으로부터 결정 경계까지의 거리가 바로 ‘마진(margin)’이다.

여기서 일단 결론을 하나 얻을 수 있다. **최적의 결정 경계는 마진을 최대화한다.**

그리고 위 그림에서는 x축과 y축 2개의 속성을 가진 데이터로 결정 경계를 그었는데, 총 3개의 데이터 포인트(서포트 벡터)가 필요했다. 즉, **n개의 속성을 가진 데이터에는 최소 n+1개의 서포트 벡터가 존재한다**는 걸 알 수 있다. 초평면의 선의 방향을 한 클래스에 2개의 데이터 포인트를 잇는 직선과 평행하게 잡는다.

## 1. 최대 마진

**큰 마진의 결정 경계를 원하는 이유는 일반화 오차가 낮아지는 경향이 있기 때문입니다.**  작은 마진의 모델은 `과대적합`되기 쉽습니다.



- 벡터 w의 길이
- ![img](https://thebook.io/img/080223/eq-156.jpg)
- 결과식

![img](https://thebook.io/img/080223/eq-157.jpg)

위 식의 좌변은 양성쪽 초평면과 음성 쪽 초평면 사이의 거리로 해석 할 수 있습니다. 이것이 최대화하려고 하는 마진입니다.

**2/|w|을 최대화함으로써 마진을 최대화 하는 것.**

모든 음성 클래스 셈플은 음성 쪽 초평면 넘어에 있어야하고 양성 클래스 샘플은 양성 쪽 초평면 너머에 있어야 합니다.

![img](https://thebook.io/img/080223/eq-159.jpg)

```python
from sklearn.svm import SVC
classifier = SVC(kernel = 'linear')
#데이터
training_points = [[1, 2], [1, 5], [2, 2], [7, 5], [9, 4], [8, 2]]
#레이블
labels = [1, 1, 1, 0, 0, 0]
#훈련
classifier.fit(training_points, labels)
#예측, 입력을 행렬로 기대한다.
print(classifier.predict([[3,2]]))

#서포트 벡터를 확인
print(classifier.support_vectors_)
```

## 2. SVM장점

대부분의 머신러닝 지도 학습 알고리즘은 학습 데이터 모두를 사용하여 모델을 학습한다. 그런데 **SVM에서는 결정 경계를 정의하는 게 결국 서포트 벡터이기 때문에 데이터 포인트 중에서 서포트 벡터만 잘 골라내면 나머지 쓸 데 없는 수많은 데이터 포인트들을 무시할 수 있다. 그래서 매우 빠르다.**



## 3. 이상치(Outlier)를 얼마나 허용할 것인가

SVM은 데이터 포인트들을 올바르게 분리하면서 마진의 크기를 최대화해야 하는데, 결국 이상치를 잘 다루는게 중요하다.

 왼쪽에 혼자 튀어 있는 파란 점과, 오른쪽에 혼자 튀어 있는 빨간 점이 있다는 걸 봐두자. 누가 봐도 **아웃라이어**다.

![img](https://i1.wp.com/hleecaster.com/wp-content/uploads/2020/01/svm06.png?fit=1024%2C768)

이제 위 아래 그림을 좀 더 자세히 비교해보자.

- 위의 그림은 아웃라이어를 허용하지 않고 기준을 까다롭게 세운 모양이다. 이걸 **하드 마진(hard margin)**이라고 부른다. 그리고 서포트 벡터와 결정 경계 사이의 거리가 매우 좁다. 즉, 마진이 매우 작아진다. 이렇게 개별적인 학습 데이터들을 다 놓치지 않으려고 아웃라이어를 허용하지 않는 기준으로 결정 경계를 정해버리면 **오버피팅(overfitting)** 문제가 발생할 수 있다.
- 아래 그림은 아웃라이어들이 마진 안에 어느정도 포함되도록 너그럽게 기준을 잡았다. 이걸 **소프트 마진(soft margin)**이라고 부른다. 이렇게 너그럽게 잡아 놓으니 서포트 벡터와 결정 경계 사이의 거리가 멀어졌다. 즉, 마진이 커진다. 대신 너무 대충대충 학습하는 꼴이라 **언더피팅(underfitting)** 문제가 발생할 수 있다.



## 파라미터 C

```python
classifier = SVC(C = 0.01)
```

C값이 클수록 하드마진(오류 허용 안 함), 작을수록 소프트마진(오류를 허용함)이다.

여러가지 C값을 넣어보면서 모델을 검증하는 수박에 없다.



## 커널(Kernel)

지금까지는 선형으로 결정 경계를 그을 수 있는 형태의 데이터세트. 만약 SVM이 선형으로 분리 할 수 없는 데이터 세트가 있다면 어떻게 해야 할까?

scikit-learn에서는 SVM모델을 만들 때 kernel을 지정하여 해결할 수 있다.

**비선형 문제를 풀기 위해 사용한다.**

```python
from sklearn.svm import SVC
classifier = SVC(kernel = 'linear')
```

kernel에 `poly`같은 걸 넣어줄 수도 있다. 다만 다른 커널을 사용할 시 주의가 필요하다. 머신러닝 모델이 약간의 오차를 허용해야 하는 건 너무나 당연한거라 단순히 outlier때문에 선형으로 분리할 수 없다고 판단해서는 안된다. 일부 아웃라이어에 맞추기 위해 비선형으로 결정 경계를 만들 필요가 없다는 뜻이다. 모든 점을 올바르게 분리하는 선을 그린다는 건 결국 모델이 데이터에 과도하게 적합해진다는 즉, 오버피팅이 된다는 거다.

- 매핑함수를 사용하여 훈련데이터를 고차원 특성 공간으로 변환합니다. 그다음 이 새로운 특성공간에서 데이터를 분류하는 선형SVM모델을 훈련합니다. 
- 동일한 매핑 함수를 사용하여 새로운 본 적 없는 데이터를 변환하고, 선형 SVM을 모델을 사용하여 분류할 수 있습니다.
- 문제점 : 새로운 특성을 만드는 계싼 비용이 매우 비싸다는 것입니다.
- 커널이란 용어를 샘플간의 유사도 함수로 해석할 수 있습니다.
- 지수 함수로 얻게되는 유사도 점수는 1(매우 비슷한 샘플) 과 0(매우 다른 샘플)사이 범위를 가집니다.

### 1. 다항식(Polynomial)

- 2차원의 점을 3차원으로 변환

![img](https://i1.wp.com/hleecaster.com/wp-content/uploads/2020/01/svm07.png?fit=1024%2C768)

아무리 봐도 단순한 선형으로는 도저히 해결이 안 된다.

이때 **다항식(polynomial)** 커널을 사용하면 2차원에서 x, y 좌표로 이루어진 점들을 아래와 같은 식에 따라 3차원으로 표현하게 된다.

![img](https://hleecaster.com/wp-content/uploads/2020/01/svm08.png)

예를 들어 [1, 2] 점이 있다고 하면 이렇게 3차원으로 계산해놓는다.

![img](https://hleecaster.com/wp-content/uploads/2020/01/svm09.png)

그리고 이렇게 다항식 커널로 계산한 데이터 포인트들을 3차원으로 그려보면 이런 모양이 나타난다.

![img](https://i1.wp.com/hleecaster.com/wp-content/uploads/2020/01/svm10.png?fit=1024%2C768)

이렇게 다항식커널을 사용하면 데이터를 더 높은 차원으로 변형하여 나타냄으로써 초평면의 결정 경계를 얻을 수 있다.



### 2. 방사 기저 함수(RBF)

RBF커널 가우시안 커널이라고 부르기도 한다.

scikit-learn에서 모델을 불러올 때 파라미터로 kernel값을 따로 안 넣어주었을 떄의 기본값이 바로 `rbf`이다.(`linear`, `poly`, 그리고 `sigmoid`)

```python
from sklearn.svm import SVC
classifier = SVC(kernel='rbf',random_state=1, gamma=0.10,C=10.0)
svm.fit(X_xor,y_xor)
plot_decision_regions(X_xor,y_xor,classifier=svm)
plt.legend(loc='best')
plt.tight_layout()
plt.show()
```

**RBF커널**은 2차원의 점울 무한한 차원의 점으로 변환한다. 시각화는 어렵고 선형대수학이 사용된다.



### 3. 파라미터 gamma

`gamma`는 파리미터다.

```python
classifier = SVC(kernerl = 'rbf', C=2, gamma=0.5)
```

gamma는 결정경계를 얼마나 유연하게 그을 것인지 정해주는 거다. 학습 데이터에 얼마나 민감하게 반응할 것인지 모델을 조정하는거니깐 C와 비슷한 개념이라 봐도 된다.

- gamma값을 높이면 학습 데이터에 많이 의존해서 결정 경계를 구불구불 긋게 된다. 이는 오버피팅을 초래
- 대로 `gamma`를 **낮추면** 학습 데이터에 별로 의존하지 않고 **결정 경계를 직선에 가깝게** 긋게 된다. 이러면 **언더피팅**이 발생할 수 있다.



### 4. 로지스틱회기 vs 서포트 벡터 머신

실제 분류 작업에서 선형 로지스틱 회귀와 선형 SVM은 종종 비슷한 결과를 만듭니다. **로지스틱 회귀는 훈련 데이터의 조건부 가능도를 최대화하기 때문에 SVM보다 이상치에 민감합니다.**

**SVM은 결정경계에 가장 가까운 포인트(서포트벡터)에 대부분 관심을 둡니다.** 반면 로지스틱 회귀는 모델이 간단하고 구현하기가 더 쉬운 장점이 있습니다. 또한 로지스틱회귀는 업데이트에 용이하다. 하지만 학습 수행 속도는 SVM은 서포트 벡터에만 치중하기 떄문에 빠르다.



### 5. 사이킷런의 다른 구현

LogisticRegression 클래스는 LIBLINEAR라이브러리, SVC클래스는 LIBSVM라이브러리를 사용.

이따금 데이터셋이 너무 커서 컴퓨터 메모리 용량에 맞지않는 경우가 있습니다. 사이킷런은 이에 대한 대안으로 SGDClassifier클래스를 제공합니다. 이 클래스는 partial_fit메서드를 사용하여 온라인 학습을 지원합니다. 확률적 경사 하강법을 사용한다.

```python
from sklearn.linear_model import SGDClassifier
ppn = SGDClassifier(loss = 'perceptron')
lr = SGDClassifier(loss = 'log')
svm = SGDClassifier(loss='hinge')
```



