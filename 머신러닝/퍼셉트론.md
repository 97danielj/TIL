[TOC]

# 1. 퍼셉트론

## 1. 인공 뉴런

- 맥컬록-피츠(MCP) : 생물학적 뇌가 동작하는 방식을 이해하려는 시도

- 퍼셉트론 : MCP 뉴런 모델 기반으로 자동으로 가중치를 학습하는 알고리즘

  - 인간의 뉴런을 수학화, 공학화
  - **자동으로 최적의 가중치를 학습**
  - 인공뉴런
  - **출력을 내거나 안내거나 하는 이진 출력을 가진다.**
  - 퍼셉트론은 두 클래수가 선형적으로 구분되고 학습률이 충분히 작을 때만 수렴합니다.

  

- 인공 뉴런

  - 1 or -1 분류되는 이진 분류 작업
  - 입력값 x : 특정 샘플 벡터. 열벡터
  - 가중치 벡터 w : 열벡터
  - **출력할 것인지의 결정함수는 단위계단 함수이다.**
  - 세타는 임계값
  - 임계값 세타를 식의 왼쪽으로 옮기면 -세타는 절편이 된다.
  - **가중치 벡터를 전치한다음 두 벡터의 내적 : z**
  - z는 최종 입력
  - z = wT.dot(x)
  - **최종 입력은 활성화함수(결정함수)를 통과하여 z가 0보다 크면 1로 그 외는 -1로 출력한다.**

## 2. 퍼셉트론 학습 규칙

퍼셉트론의 알고리즘을 요약하면 다음의 과정과 같습니다.

1. 가중치를 0또는 랜덤한 작은값으로 초기화
2. 각 훈련 샘플 x에서 다음 작업을 합니다.
   1. 출력 값 y^를 계산합니다.
   2. 가중치를 업데이트합니다.

^y는 단위계단함수(결정함수)로 예측한 클래스 레이블입니다.

- **가중치 업데이트 식 : 가중치j = 가중치j+가중치변화율j**

- **가중치변화율 = 학습률*(실제-예측값)입력값j**
- 가중치 변화율은 예측값과의 차이 입력값의 크기에 비례해서 더 크게 변경된다.
- 각 가중치 차원에 대해서 업데이트가 동시에 진행된다.
- 가중치 벡터의 모든 가중치를 동시에 업데이트 한다. 즉 모든 가중치가 각자의 업데이트값에 의해 업데이트 되기 전에는 예측 레이블 ^y를 계산하지 않습니다.
- **퍼셉트론이 정확히 클래스 레이블을 예측한 경우는 가중치 변경없이 그대로 유지**
- **잘못 예측한 경우 가중치를 양성 또는 음성 타깃 클래스 방향으로 이동시킵니다.**

- 에포크 : 훈련 데이터셋을 반복할 최대 휫수
- 두 클래스를 선형 결정 경계로 나눌 수 없다면 에포크를 지정하여 분류 허용 오차를 줄인다.
- 그렇지 않으면 계속된 가중치 업데이트 일어날 것이다.



# 2. 적응형 선형뉴런

## 1. 개념

단일층 신경망의 또 다른 종류인 적응형 선형 뉴런을 살펴보겠습니다. 

아달린은 퍼셉트론의 향상된 버전

아달린은 연속함수로 비용함수를 정의하 최소화하는 핵심개념을 보여준다.

|       차이점        |             아달린 규칙              |                       퍼셉트론                       |
| :-----------------: | :----------------------------------: | :--------------------------------------------------: |
|   가중치 업데이트   |      선형활성화 함수(항등함수)       |                     단위계단함수                     |
|      최종 예측      |          임계함수(단위함수)          |                     단위계단함수                     |
| 모델 오차 계산 지점 | 진짜 클래스 레이블 - 선형활성화 함수 | 진짜 클래스 레이블 - 단위계단함수(에측클래스 레이블) |



아달린은 계산된 출력과 진짜 클래스레이블 사이의 제곱 오차합으로 가중치를 학습하기 위한 비용함수 J를 정의합니다.

$$
비용함수(목적함수) : J(w)=\frac{1}{2}\sum_{i}(y^i-에측값)^2
$$

  - 1/2항은 다음 문단에 설명할 비용함수의 그레이디언트를 간소하게 만든것
- **비용함수의 그라디언트를 최소화값을 찾아 비용함수의 최소값을 찾는다.**
- 단위계단함수 대신 연속적인 선형활성화 함수를 사용하는 장점은 비용함수가 미분가능해진다는 것
- 또한 비용함수는 볼록함수이다.
- 경사하강법을 통해 비용함수를 최소화하는 가중치를 찾을 수 있습니다.
- 진행은 경사의 반대 방향으로 진행 
- 진행 크기는 경사의 기울기와 학습률로 결정합니다. 학습률이 stepsize이다.
- 퍼셉트론처럼 개별 훈련 샘플마다 평가한 후 가중치를 업데이트하지 않고 전체 훈련 데이터셋을 기반으로 그레이디언트를 계싼합니다.
- X.dot(erros) : 전체 데이터셋에 대해서 가중치를 업데이트 한다.





# 3. 경사 하강법

## 1. 개념

1차 근사값 발견용 최적화 알고리즘. 기본 개념은 **함수의 기울기(경사)를 구하고 경사의 절댓값이 낮은쪽으로 계속 이동시켜 극값에 이를 때 까지 반복시키는 것**



## 2. 머신러닝에서 쓰이는 이유

함수의 최솟값을 찾고자 할 때, 미분계수를 구함으로써 찾곤한다. 컴픁에서는 해당방법을 사용하지 않는 이유는 다음과 같다.

- 실제 분석에서(특히, 딥러닝 알고리즘을 활용하는 경우) 보게 되는 함수들은 형태가 굉장히 복잡해서 **미분계수와 그 근을 계산하기 어려운 경우가 많음**
- 미분계수 계산 과정을 컴퓨터로 구현하는 것보다, **경사하강법을 구현하는 것이 훨씬 쉬움**
- 데이터의 양이 매우 큰 경우 경사하강법과 같은 순차적인 방법이 **계산량 측면에서 훨씬 효율적**

지도 학습 알고리즘의 핵심 구성요소중 하나는 **학습과정동안 최적화하기 위해 정의한 목적함수입니다.** **종종 최소화 하려는 비용함수가 목적함수가 됩니다.**



## 3. 공식 유도

경사하강법은 **함수의 기울기(=gradient)를 이용해서 함수의 최소값 일 때의 x값을 찾기 위한 방법**이다. 기울기가 양수인 경우는 *x* 값이 증가할수록 함수 값도 증가하고, 반대로 음수인 경우에는 *x* 값이 증가할수록 함수 값이 감소한다. 그리고 기울기 값이 크다, 기울기가 가파르다는 것은 최소값으로부터 거리가 멀다는 뜻이다. 극값에서 주로 최솟값을 가진다.

이 점을 이용해서 **기울기가 양수라면 음의 방향**으로 *x*를 옮기면 되고, **기울기가 음수라면 양의 방향**으로 *x*를 옮기면 된다.
$$
x 
_{i+1}
​
 =x 
_{i}
​
 −이동거리×기울기의 부호
$$
식에서는 기울기의 부호는 알 수 있지만. 이동거리는?

미분 계수는 극소값에 가까워 질수록 값이 작아진다. 따라서, **이동거리에는 미분계수와 비례하는 값을 이용한다.** 그럼 극소값에서 멀 때는 많이 이동하고, 극소값에 가까울 때는 조금씩 이동할 수 있다.
$$
x _{i+1}=x _{i}-a{\operatorname{df}\over\operatorname{dx}}
$$

- **Step size(= *α*)**

**step size 선택은 매우 중요**하다. Step size가 큰 경우 이동 거리가 커지므로 빠르게 수렴할 수 있다는 장점이 있지만, 최소값으로 수렴되지 못 하고 함수값이 발산할 여지가 있다.
한편, step size가 너무 작으면 발산하지는 않겠지만, 최소값을 찾는데 너무 오래 걸릴 여지가 있다.
![img](https://velog.velcdn.com/images%2Fsasganamabeer%2Fpost%2F347807c0-c93e-42e3-ad63-ddb26ef9b8e6%2Fimage.png)

- **Local minima**

우리가 찾고 싶은 건 global minima(함수 전체에서의 최소값)이지만, 어떤 경우에는 local minima에 빠져 벗어나지 못 하는 상황이 발생한다.
![img](https://velog.velcdn.com/images%2Fsasganamabeer%2Fpost%2F89b4009c-7c5f-4745-960d-fba9845acea6%2Fimage.png)

하지만 최근에는 **실제로 딥러닝이 수행될 때, local minima에 빠질 확률이 거의 없다**고 한다.
위의 그래프는 가중치(w)가 1개인 모델이지만 실제 딥러닝 모델에서는 w가 수도 없이 많고, 그 수 많은 w가 모두 local minima에 빠져야 w 업데이트가 정지된다. 이론적으로 거의 불가능에 가까운 일이므로, 사실상 local minima는 고려할 필요가 없다는 것이 중론이다.



### 4. 특성 스케일을 조정하여 경사 하강법 결과 향상

표준화 - 가장 대표적으로 사용되는 스케일링

이 정규화 과정은 데이터에 평균이 0이고, 단위분산을 갖는 표준 정규 분포의 성질을 부여하여 경사 하강법학습이 좀 더 빠르게 수렴되도록 돕습니다.

![특성스케일조정](https://git.io/JtIbB)



## 4. 배치경사 하강법

GPU가 CPU보다 유리한 점은 병렬 연산이다. **배치는 GPU가 한번에 처리하는 데이터의 묶음**을 의미한다.

전체학습 데이터를 하나의 배치로(배치 크기가 n)묶어 학습시키는 경사 하강법

전체 데이터에 대한 모델의 오차의 평균율 구한 다음 , 이를 이용하여 미분을 통해 경사를 산출, 최적화를 진행한다.

**배치는 1 epoch당 사용되는 training dataset의 묶음이며, epoch의 의미는 훈련 세트를 한 번 모두 사용하는 과정을 의미합니다.**

### 배치 경사 하강법의 특징

- BGD은 한 스텝에 전체 데이터를 이용하기 때문에 연산 횟수가 적습니다.(1 epoch 당 1회 update)
- 전체 데이터를 모두 한 번에 처리하기 때문에, 메모리가 가장 많이 필요하다.
- 항상 같은 데이터 (전체 데이터)에 대해 경사를 구하기 때문에, 수렴이 안정적이다. (아래 그림 참고)
- 최적해에 대한 수렴이 안정적으로 진행됩니다.
- 보통 딥러닝 라이브러리에서 배치를 지정하지 않으면 이 방법을 쓰고 있다고 생각할 수 있다.
- 여기서의 **배치(Batch)**의 의미는 *전체 데이터셋*을 의미합니다.

![image-20220522192947087](분류알고리즘1.assets/image-20220522192947087.png)



## 5. 확률적 경사 하강법 (Stochastic Gradient Descent: SGD)

- 확률적 경사 하강법은 전체 데이터 중 **단 하나의 데이터를 이용하여 경사 하강법을 1회 진행(배치 크기가 1)**하는 방법이다.

- 전체 학습 데이터 중 랜덤하게 선택된 하나의 데이터로 학습을 하기 때문에 확률적 이라 부른다.

- 데이터로 학습할 수 있고, 속도가 빠른 장점이 있다. 무엇보다 큰 특징은 **수렴에 Shooting이 발생**한다는 점이다.

- 수렴속도는 빠르지만 각 데이터에 대한 손실값의 기울기는 약간씩 다르기 때문에 shooting 이 발생한다.
- 그러나 결국 학습 데이터 전체에 대해 보편적으로 좋은 값을 내는 방향으로 수렴한다. 다만, 최저점에 안착하기는 어렵다.

![image-20220522193006869](분류알고리즘1.assets/image-20220522193006869.png)

- 온라인 학습으로 사용할 수도 있다.
  - 새로운 훈령 데이터가 도착하는 대로 훈련이 됩니다.
  - 예를 들어 고객 데이터를 처리하는 웹 애플리케이션입니다.
  - 온라인 학습을 사용해서 시스템은 변화에 즉시 적응합니다.
