[toc]

# 02) 버트(Bidirectional Encoder Representations from Transformers, BERT)

![img](https://wikidocs.net/images/page/35594/BERT.PNG)

BERT(Bidirectional Encoder Representations from Transformers)는 2018년에 구글이 공개한 사전 훈련된 모델입니다. BERT라는 이름은 세서미 스트리트라는 미국 인형극의 케릭터 이름이기도 한데, 앞서 소개한 임베딩 방법론인 ELMo와 마찬가지로 세서미 스트리트의 케릭터 이름을 따온 것이기도 합니다.

BERT는 2018년에 공개되어 등장과 동시에 수많은 NLP 태스크에서 최고 성능을 보여주면서 명실공히 NLP의 한 획을 그은 모델로 평가받고 있습니다. 이번 챕터에서는 BERT의 구조에 대해 이해하고, 이어서 BERT를 실습해보겠습니다.

간단한 BERT 실습을 위해 아래와 같이 transformers라는 패키지를 설치하겠습니다.

```python
pip install transformers
```

## **1. BERT의 개요**

![img](https://wikidocs.net/images/page/35594/%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D.PNG)

BERT는 이전 챕터에서 배웠던 트랜스포머를 이용하여 구현되었으며, 위키피디아(25억 단어)와 BooksCorpus(8억 단어)와 같은 레이블이 없는 텍스트 데이터로 사전 훈련된 언어 모델입니다.

BERT가 높은 성능을 얻을 수 있었던 것은, 레이블이 없는 방대한 데이터로 사전 훈련된 모델을 가지고, 레이블이 있는 다른 작업(Task)에서 추가 훈련과 함께 하이퍼파라미터를 재조정하여 이 모델을 사용하면 성능이 높게 나오는 기존의 사례들을 참고하였기 때문입니다. 다른 작업에 대해서 파라미터 재조정을 위한 추가 훈련 과정을 **파인 튜닝(Fine-tuning)**이라고 합니다.

위의 그림은 BERT의 파인 튜닝 사례를 보여줍니다. 우리가 하고 싶은 태스크가 스팸 메일 분류라고 하였을 때, 이미 위키피디아 등으로 사전 학습된 BERT 위에 분류를 위한 신경망을 한 층 추가합니다. 이 경우, 비유하자면 BERT가 언어 모델 사전 학습 과정에서 얻은 지식을 활용할 수 있으므로 스팸 메일 분류에서 보다 더 좋은 성능을 얻을 수 있습니다.

이전에 언급한 ELMo나 OpenAI GPT-1 등이 이러한 파인 튜닝 사례의 대표적인 예입니다.

> 1. 레이블이 없는 방대한 데이터로 사전 훈련 언어 모델
> 2. 레이블이 있는 다른 작업 추가 훈련(하이퍼 파라미터 재조정)- > **파인 튜닝**



## **2. BERT의 크기**

![img](https://wikidocs.net/images/page/35594/bartbase%EC%99%80large.PNG)

BERT의 기본 구조는 트랜스포머의 인코더를 쌓아올린 구조입니다. Base 버전에서는 총 12개를 쌓았으며, Large 버전에서는 총 24개를 쌓았습니다. 그 외에도 Large 버전은 Base 버전보다 $d_{model}$의 크기나 셀프 어텐션 헤드(Self Attention Heads)의 수가 더 큽니다. 트랜스포머 인코더 층의 수를 L, d_model의 크기를 D, 셀프 어텐션 헤드의 수를 A라고 하였을 때 각각의 크기는 다음과 같습니다.

> 멀티 헤드 : 셀프 어텐션을 병렬적을 수행
>
> 인코더 layers : 인코더의 개수

- BERT-Base : L=12, D=768, A=12 : 110M개의 파라미터
- BERT-Large : L=24, D=1024, A=16 : 340M개의 파라미터

초기 트랜스포머 모델(https://wikidocs.net/31379)이 L=6, D=512, A=8이었다는 것과 비교하면 Base 또한 초기 트랜스포머보다는 큰 네트워크임을 알 수 있습니다. 여기서 BERT-base는 BERT보다 앞서 등장한 Open AI GPT-1과 하이퍼파라미터가 동일한데, 이는 BERT 연구진이 직접적으로 GPT-1과 성능을 비교하기 위해서 GPT-1과 동등한 크기로 BERT-Base를 설계하였기 때문입니다. 반면, BERT-Large는 BERT의 최대 성능을 보여주기 위해 만들어진 모델입니다. BERT가 세운 기록들은 대부분 BERT-Large를 통해 이루어졌습니다.

이 글에서는 앞으로 편의를 위해 BERT-BASE를 기준으로 설명합니다.

## **2. BERT의 크기**

![img](https://wikidocs.net/images/page/35594/bartbase%EC%99%80large.PNG)

BERT의 기본 구조는 트랜스포머의 인코더를 쌓아올린 구조입니다. Base 버전에서는 총 12개를 쌓았으며, Large 버전에서는 총 24개를 쌓았습니다. 그 외에도 Large 버전은 Base 버전보다 $d_{model}$의 크기나 셀프 어텐션 헤드(Self Attention Heads)의 수가 더 큽니다. 트랜스포머 인코더 층의 수를 L, $d_{model}$의 크기를 D, 셀프 어텐션 헤드의 수를 A라고 하였을 때 각각의 크기는 다음과 같습니다.

- BERT-Base : L=12, D=768, A=12 : 110M개의 파라미터
- BERT-Large : L=24, D=1024, A=16 : 340M개의 파라미터

초기 트랜스포머 모델(https://wikidocs.net/31379)이 L=6, D=512, A=8이었다는 것과 비교하면 Base 또한 초기 트랜스포머보다는 큰 네트워크임을 알 수 있습니다. 여기서 BERT-base는 BERT보다 앞서 등장한 Open AI GPT-1과 하이퍼파라미터가 동일한데, 이는 BERT 연구진이 직접적으로 GPT-1과 성능을 비교하기 위해서 GPT-1과 동등한 크기로 BERT-Base를 설계하였기 때문입니다. 반면, BERT-Large는 BERT의 최대 성능을 보여주기 위해 만들어진 모델입니다. BERT가 세운 기록들은 대부분 BERT-Large를 통해 이루어졌습니다.

이 글에서는 앞으로 편의를 위해 BERT-BASE를 기준으로 설명합니다.

## 3. BERT의 문맥을 반영한 임베딩(Contextual Embedding)

BERT는 ELMo나 GPT-1과 마찬가지로 **문맥을 반영한 임베딩**(Contextual Embedding)을 사용하고 있습니다.

![img](https://wikidocs.net/images/page/115055/bert0.PNG)

BERT의 입력은 앞서 배운 딥 러닝 모델들과 마찬가지로 임베딩 층(Embedding layer)를 지난 임베딩 벡터들입니다. $d_{model}$을 768로 정의하였으므로, 모든 단어들은 768차원의 임베딩 벡터가 되어 BERT의 입력으로 사용됩니다. BERT는 내부적인 연산을 거친 후, 동일하게 각 단어에 대해서 768차원의 벡터를 출력합니다. 위의 그림에서는 BERT가 각 768차원의 [CLS], I, love, you라는 4개의 벡터를 입력받아서(입력 임베딩) 동일하게 768차원의 4개의 벡터를 출력하는 모습(출력 임베딩)을 보여줍니다.

![img](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC2.PNG)

BERT의 연산을 거친 후의 출력 임베딩은 문장의 문맥을 모두 참고한 문맥을 반영한 임베딩이 됩니다. 위의 좌측 그림에서 [CLS]라는 벡터는 BERT의 초기 입력으로 사용되었을 입력 임베딩 당시에는 단순히 임베딩 층(embedding layer)를 지난 임베딩 벡터였지만, BERT를 지나고 나서는 [CLS], I, love, you라는 모든 단어 벡터들을 모두 참고한 후에 문맥 정보를 가진 벡터가 됩니다. 위의 좌측 그림에서는 모든 단어를 참고하고 있다는 것을 점선의 화살표로 표현하였습니다.

> BERT 연산 후 입력으로 들어온 임베딩 벡토는 모든 단어 벡터들을 참고한 후에 문맥 정보를 가진 벡터가 됩니다.

이는 [CLS]라는 단어 벡터 뿐만 아니라 다른 벡터들도 전부 마찬가지입니다. 가령, 우측의 그림에서 출력 임베딩 단계의 love를 보면 BERT의 입력이었던 모든 단어들인 [CLS], I, love, you를 참고하고 있습니다.

![img](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC3.PNG)

하나의 단어가 모든 단어를 참고하는 연산은 사실 BERT의 12개의 층에서 전부 이루어지는 연산입니다. 그리고 이를 12개의 층을 지난 후에 최종적으로 출력 임베딩을 얻게되는 것입니다. 가령, 위의 그림은 BERT의 첫번째 층에 입력된 각 단어가 모든 단어를 참고한 후에 출력되는 과정을 화살표로 표현하였습니다. BERT의 첫번째 층의 출력 임베딩은 BERT의 두번째 층에서는 입력 임베딩이 됩니다.

![img](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC4.PNG)

그렇다면 BERT는 어떻게 모든 단어들을 참고하여 문맥을 반영한 출력 임베딩을 얻게 되는 것일까요? 사실 이에 대한 해답을 여러분은 이미 알고있는데, 바로 '셀프 어텐션'입니다. BERT는 기본적으로 트랜스포머 인코더를 12번 쌓은 것이므로 내부적으로 각 층마다 멀티 헤드 셀프 어텐션과 포지션 와이즈 피드 포워드 신경망을 수행하고 있습니다.

## **4. BERT의 서브워드 토크나이저 : WordPiece**

BERT는 단어보다 더 작은 단위로 조개는 서브워드 토크나이저를 사용합니다. BERT가 사용한 토크나이저는 WordPiece토크나이저로 서브워드 토크나이저 챕터에서 공부한 바이트 페어 인코딩(PBE)의 유사 알고리즘입니다.  동작 방식은 BPE와 조금 다르지만, 글자로부터 서브워드들을 병합해가는 방식으로 최종 단어 집합(Vocabulary)을 만드는 것은 BPE와 유사합니다.

서브워드 토크나이저는 기본적으로 자주 등장하는 단어는 그대로 단어 집합에 추가하지만, 자주 등장하지 않는 단어의 경우에는 더 작은 단위인 서브워드로 분리되어 서브워드들이 단어 집합에 추가된다는 아이디어를 갖고있습니다. 이렇게 단어 집합이 만들어지고 나면, 이 단어 집합을 기반으로 토큰화를 수행합니다. 이는 대표적인 서브워드 토크나이저 패키지인 SentencePiece 실습을 통해 이해한 내용입니다. BERT의 서브워드 토크나이저도 이와 마찬가지로 동작합니다.

BERT에서 토큰화를 수행하는 방식은 다음과 같습니다.

```markdown
준비물 : 이미 훈련 데이터로부터 만들어진 단어 집합
1. 토큰이 단어 집합에 존재한다.
=> 해당 토큰을 분리하지 않는다.
2. 토큰이 단어집합에 존재하지 않는다.
=> 해당 토큰을 서브워드로 분리한다.
=> 해당 토큰의 첫번째 서브워드를 제외한 나머지 서브워드들은 앞에 '##'를 붙인것을 토큰으로 한다.
```

예를 들어 embeddings이라는 단어가 입력으로 들어왔을 때, BERT의 단어 집합에 해당 단어가 존재하지 않았다고 해봅시다. 만약, 서브워드 토크나이저가 아닌 토크나이저라면 여기서 OOV 문제가 발생합니다. 하지만 서브워드 토크나이저의 경우에는 해당 단어가 단어 집합에 존재하지 않았다고 해서, 서브워드 또한 존재하지 않는다는 의미는 아니므로 해당 단어를 더 쪼개려고 시도합니다. 만약, BERT의 단어 집합에 em, ##bed, ##ding, ##s라는 서브 워드들이 존재한다면, embeddings는 em, ##bed, ##ding, ##s로 분리됩니다. 여기서 ##은 이 서브워드들은 단어의 중간부터 등장하는 서브워드라는 것을 알려주기 위해 단어 집합 생성 시 표시해둔 기호입니다. 이런 표시가 있어야만 em, ##bed, ##ding, #s를 다시 손쉽게 embeddings로 복원할 수 있을 것입니다.

참고로 BERT에서 사용되는 특별 토큰들과 그와 맵핑되는 정수는 다음과 같습니다.

- [PAD] - 0
- [UNK] - 100
- [CLS] - 101
- [SEP] - 102
- [MASK] - 103

## **5. 포지션 임베딩(Position Embedding)**

트랜스포머에서는 포지셔널 인코딩(Positional Encoding)이라는 방법을 통해서 단어의 위치 정보를 표현했습니다. 포지셔널 인코딩은 사인 함수와 코사인 함수를 사용하여 위치에 따라 다른 값을 가지는 행렬을 만들어 이를 단어 벡터들과 더하는 방법입니다. BERT에서는 이와 유사하지만, 위치 정보를 사인 함수와 코사인 함수로 만드는 것이 아닌 학습을 통해서 얻는 포지션 임베딩(Position Embedding)이라는 방법을 사용합니다.

> 버트 인코더 : 문맥정보 + 단어 위치 정보

![img](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC5.PNG)

위의 그림은 포지션 임베딩을 사용하는 방법을 보여줍니다. 우선, **위의 그림에서 WordPiece Embedding은 우리가 이미 알고 있는 단어 임베딩으로 실질적인 입력입니다. 그리고 이 입력에 포지션 임베딩을 통해서 위치 정보를 더해주어야 합니다**. 포지션 임베딩의 아이디어는 굉장히 간단한데, 위치 정보를 위한 임베딩 층(Embedding layer)을 하나 더 사용합니다. 가령, 문장의 길이가 4라면 4개의 포지션 임베딩 벡터를 학습시킵니다. 그리고 BERT의 입력마다 다음과 같이 포지션 임베딩 벡터를 더해주는 것입니다.

- 첫번째 단어의 임베딩 벡터 + 0번 포지션 임베딩 벡터
- 두번째 단어의 임베딩 벡터 + 1번 포지션 임베딩 벡터
- 세번째 단어의 임베딩 벡터 + 2번 포지션 임베딩 벡터
- 네번째 단어의 임베딩 벡터 + 3번 포지션 임베딩 벡터

실제 BERT에서는 문장의 최대 길이를 512로 하고 있으므로, 총 512개의 포지션 임베딩 벡터가 학습됩니다. 결론적으로 현재 설명한 내용을 기준으로는 BERT에서는 총 두 개의 임베딩 층이 사용됩니다. 단어 집합의 크기가 30,522개인 단어 벡터를 위한 임베딩 층과 문장의 최대 길이가 512이므로 512개의 포지션 벡터를 위한 임베딩 층입니다.

사실 BERT는 세그먼트 임베딩(Segment Embedding)이라는 1개의 임베딩 층을 더 사용합니다. 이에 대해서는 뒤에 언급합니다.

## **6. BERT의 사전 훈련(Pre-training)**

![img](https://wikidocs.net/images/page/35594/bert-openai-gpt-elmo-%EC%B6%9C%EC%B2%98-bert%EB%85%BC%EB%AC%B8.png)

위의 그림은 BERT의 논문에 첨부된 그림으로 ELMo와 GPT-1, 그리고 BERT의 구조적인 차이를 보여줍니다. 가장 우측 그림의 ELMo는 정방향 LSTM과 역방향 LSTM을 각각 훈련시키는 방식으로 양방향 언어 모델을 만들었습니다. 가운데 그림의 GPT-1은 트랜스포머의 디코더를 이전 단어들로부터 다음 단어를 예측하는 방식으로 단방향 언어 모델을 만들었습니다. Trm은 트랜스포머를 의미합니다. 단방향(→)으로 설계된 Open AI GPT와 달리 가장 좌측 그림의 BERT는 화살표가 양방향으로 뻗어나가는 모습을 보여줍니다. 이는 마스크드 언어 모델(Masked Language Model)을 통해 양방향성을 얻었기 때문입니다. BERT의 사전 훈련 방법은 크게 두 가지로 나뉩니다. 첫번째는 마스크드 언어 모델이고, 두번째는 다음 문장 예측(Next sentence prediction, NSP)입니다.

논문에 따르면 BERT는 BookCorpus(8억 단어)와 위키피디아(25억 단어)로 학습되었습니다.

### **1) 마스크드 언어 모델(Masked Language Model, MLM)**

BERT는 사전 훈련을 위해서 인공 신경망의 입력으로 들어가는 입력 텍스트의 15%의 단어를 랜덤으로 마스킹(Masking)합니다. 그리고 인공 신경망에게 이 가려진 단어들을(Masked words) 예측하도록 합니다. 중간에 단어들에 구멍을 뚫어놓고, 구멍에 들어갈 단어들을 예측하게 하는 식입니다. 예를 들어 '나는 [MASK]에 가서 그곳에서 빵과 [MASK]를 샀다'를 주고 '슈퍼'와 '우유'를 맞추게 합니다.

더 정확히는 전부 [MASK]로 변경하지는 않고, 랜덤으로 선택된 15%의 단어들은 다시 다음과 같은 비율로 규칙이 적용됩니다.

- 80%의 단어들은 [MASK]로 변경한다.
  Ex) The man went to the store → The man went to the [MASK]
- 10%의 단어들은 랜덤으로 단어가 변경된다.
  Ex) The man went to the store → The man went to the dog
- 10%의 단어들은 동일하게 둔다.
  Ex) The man went to the store → The man went to the store

이렇게 하는 이유는 [MASK]만 사용할 경우에는 [MASK] 토큰이 파인 튜닝 단계에서는 나타나지 않으므로 사전 학습 단계와 파인 튜닝 단계에서의 불일치가 발생하는 문제가 있습니다. 이 문제을 완화하기 위해서 랜덤으로 선택된 15%의 단어들의 모든 토큰을 [MASK]로 사용하지 않습니다. 이를 전체 단어 관점에서 그래프를 통해 정리하면 다음과 같습니다.

![img](https://wikidocs.net/images/page/115055/%EC%A0%84%EC%B2%B4%EB%8B%A8%EC%96%B4.PNG)

**전체 단어의 85%는 마스크드 언어 모델의 학습에 사용되지 않습니다. 마스크드 언어 모델의 학습에 사용되는 단어는 전체 단어의 15%입니다.** 학습에 사용되는 12%는 [MASK]로 변경 후에 원래 단어를 예측합니다. 1.5%는 랜덤으로 단어가 변경된 후에 원래 단어를 예측합니다. 1.5%는 단어가 변경되지는 않았지만, BERT는 이 단어가 변경된 단어인지 원래 단어가 맞는지는 알 수 없습니다. 이 경우에도 BERT는 원래 단어가 무엇인지를 예측하도록 합니다.

예시를 통해 이해해봅시다. 'My dog is cute. he likes playing'이라는 문장에 대해서 마스크드 언어 모델을 학습하고자 합니다. 약간의 전처리와 BERT의 서브워드 토크나이저에 의해 이 문장은 ['my', 'dog', 'is' 'cute', 'he', 'likes', 'play', '##ing']로 토큰화가 되어 BERT의 입력으로 사용됩니다. 그리고 언어 모델 학습을 위해서 다음과 같이 데이터가 변경되었다고 가정해봅시다.

- 'dog' 토큰은 [MASK]로 변경되어습니다.

![img](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC8.PNG)

위 그림은 'dog' 토큰이 [MASK]로 변경되어서 BERT 모델이 원래 단어를 맞추려고 하는 모습을 보여줍니다. 여기서 출력층에 있는 다른 위치의 벡터들은 예측과 학습에 사용되지 않고, 오직 'dog' 위치의 출력층의 벡터만이 사용됩니다. 구체적으로는 BERT의 손실 함수에서 다른 위치에서의 예측은 무시합니다. 출력층에서는 예측을 위해 단어 집합의 크기만큼의 밀집층(Dense layer)에 소프트맥스 함수가 사용된 1개의 층을 사용하여 원래 단어가 무엇인지를 맞추게 됩니다. 그런데 만약 'dog'만 변경된 것이 아니라 다음과 같이 데이터셋이 변경되었다면 어떨까요? 이번에는 세 가지 유형 모두에 대해서 가정해봅시다.

> 학습과 예측에 오직 출력층의 'dog'위치의 벡터만이 사용된다.

- 'dog' 토큰은 [MASK]로 변경되었습니다.
- 'he'는 랜덤 단어 'king'으로 변경되었습니다.
- 'play'는 변경되진 않았지만 예측에 사용됩니다.

![img](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC9.PNG)

BERT는 랜던 단어 'king'으로 변경된 토큰에 대해서도 원래 단어가 무엇인지, 변경되지 않은 단어 'play'에 대해서도 원래 단어가 무엇인지를 예측해야 합니다. 'play'는 변경되지 않았지만 BERT 입장에서는 이것이 변경된 단어인지 아닌지 모르므로 마찬가지로 원래 단어를 예측해야 합니다. BERT는 마스크드 언어 모델 외에도 다음 문장 예측이라는 또 다른 태스크를 학습합니다.

### **2) 다음 문장 예측(Next Sentence Prediction, NSP)**

BERT는 두 개의 문장을 준 후에 이 문장이 이어지는 문장인지 아닌지를 맞추는 방식으로 훈련시킵니다. 이를 위해서 50:50 비율로 실제 이어지는 두 개의 문장과 랜덤으로 이어붙인 두 개의 문장을 주고 훈련시킵니다. 이를 각각 Sentence A와 Sentence B라고 하였을 때, 다음의 예는 문장의 연속성을 확인한 경우와 그렇지 않은 경우를 보여줍니다.

- 이어지는 문장의 경우
  Sentence A : The man went to the store.
  Sentence B : He bought a gallon of milk.
  Label = IsNextSentence
- 이어지는 문장이 아닌 경우 경우
  Sentence A : The man went to the store.
  Sentence B : dogs are so cute.
  Label = NotNextSentence

![img](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC10.PNG)

BERT의 입력으로 넣을 때에는 [SEP]라는 특별 토큰을 사용해서 문장을 구분합니다. 첫번째 문장의 끝에 [SEP] 토큰을 넣고, 두번째 문장이 끝나면 역시 [SEP] 토큰을 붙여줍니다. 그리고 이 두 문장이 실제 이어지는 문장인지 아닌지를 [CLS] 토큰의 위치의 출력층에서 이진 분류 문제를 풀도록 합니다. [CLS] 토큰은 BERT가 분류 문제를 풀기 위해 추가된 특별 토큰입니다. 그리고 위의 그림에서 나타난 것과 같이 마스크드 언어 모델과 다음 문장 예측은 따로 학습하는 것이 아닌 loss를 합하여 **학습이 동시에 이루어집니다.**

BERT가 언어 모델 외에도 다음 문장 예측이라는 태스크를 학습하는 이유는 BERT가 풀고자 하는 태스크 중에서는 QA(Question Answering)나 NLI(Natural Language Inference)와 같이 두 문장의 관계를 이해하는 것이 중요한 태스크들이 있기 때문입니다.

## **7. 세그먼트 임베딩(Segment Embedding)**

![img](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC7.PNG)

앞서 언급했듯이 BERT는 QA 등과 같은 두 개의 문장 입력이 필요한 태스크를 풀기도 합니다. 문장 구분을 위해서 BERT는 세그먼트 임베딩이라는 또 다른 임베딩 층(Embedding layer)을 사용합니다. 첫번째 문장에는 Sentence 0 임베딩, 두번째 문장에는 Sentence 1 임베딩을 더해주는 방식이며 임베딩 벡터는 두 개만 사용됩니다.

결론적으로 BERT는 총 3개의 임베딩 층이 사용됩니다.

- WordPiece Embedding : 실질적인 입력이 되는 워드 임베딩. 임베딩 벡터의 종류는 단어 집합의 크기로 30,522개.
- Position Embedding : 위치 정보를 학습하기 위한 임베딩. 임베딩 벡터의 종류는 문장의 최대 길이인 512개.
- Segment Embedding : 두 개의 문장을 구분하기 위한 임베딩. 임베딩 벡터의 종류는 문장의 최대 개수인 2개.

주의할 점은 많은 문헌에서 BERT가 문장 중간의 [SEP] 토큰과 두 종류의 세그먼트 임베딩을 통해서 두 개의 문장을 구분하여 입력받을 수 있다고 설명하고 있지만, 여기서 BERT에 두 개의 문장이 들어간다는 표현에서의 문장이라는 것은 실제 우리가 알고 있는 문장의 단위는 아닙니다. 예를 들어 QA 문제를 푸는 경우에는 [SEP]와 세그먼트 임베딩을 기준으로 구분되는 [질문(Question), 본문(Paragraph)] 두 종류의 텍스트를 입력받지만, Paragraph 1개는 실제로는 다수의 문장으로 구성될수 있습니다. 다시 말해 [SEP]와 세그먼트 임베딩으로 구분되는 BERT의 입력에서의 두 개의 문장은 실제로는 두 종류의 텍스트, 두 개의 문서일 수 있습니다.

BERT가 두 개의 문장을 입력받을 필요가 없는 경우도 있습니다. 예를 들어 네이버 영화 리뷰 분류나 IMDB 리뷰 분류와 같은 감성 분류 태스크에서는 한 개의 문서에 대해서만 분류를 하는 것이므로, 이 경우에는 BERT의 전체 입력에 Sentence 0 임베딩만을 더해줍니다. 아래에서 설명할 1)과 2) 파인 튜닝 유형이 그 예라고 할 수 있겠습니다.



## 8. BERT를 파인 튜닝(Fine-tuning)하기

이번에는 사전 학습 된 BERT에 우리가 풀고자 하는 태스크의 데이터를 추가로 학습 시켜서 테스트하는 단계인 파인 튜닝 단계에 대해서 알아보겠습니다. 실질적으로 태스크에 BERT를 사용하는 단계에 해당됩니다.

### 1) 하나의 텍스트에 대한 텍스트 분류 유형(Single Text Classification)

![img](https://wikidocs.net/images/page/115055/apply1.PNG)

BERT를 사용하는 첫번째 유형은 하나의 문서에 대한 텍스트 분류 유형입니다. 이 유형은 영화 리뷰 감성 분류, 로이터 뉴스 분류 등과 같이 입력된 문서에 대해서 분류를 하는 유형으로 문서의 시작에 [CLS] 라는 토큰을 입력합니다. 앞서 사전 훈련 단계에서 다음 문장 예측을 설명할 때, [CLS] 토큰은 BERT가 분류 문제를 풀기위한 특별 토큰이라고 언급한 바 있습니다. 이는 BERT를 실질적으로 사용하는 단계인 파인 튜닝 단계에서도 마찬가지입니다. 텍스트 분류 문제를 풀기 위해서 [CLS] 토큰의 위치의 출력층에서 밀집층(Dense layer) 또는 같은 이름으로는 완전 연결층(fully-connected layer)이라고 불리는 층들을 추가하여 분류에 대한 예측을 하게됩니다.

### 2) 하나의 텍스트에 대한 태깅 작업(Tagging)

![img](https://wikidocs.net/images/page/115055/apply2.PNG)

BERT를 사용하는 두번째 유형은 태깅 작업입니다. 앞서 RNN 계열의 신경망들을 이용해서 풀었던 태스크입니다. 대표적으로 문장의 각 단어에 품사를 태깅하는 품사 태깅 작업과 개체를 태깅하는 개체명 인식 작업이 있습니다. 출력층에서는 입력 텍스트의 각 토큰의 위치에 밀집층을 사용하여 분류에 대한 예측을 하게 됩니다.

### 3) 텍스트의 쌍에 대한 분류 또는 회귀 문제(Text Pair Classification or Regression)

![img](https://wikidocs.net/images/page/115055/apply3.PNG)

BERT는 텍스트의 쌍을 입력으로 받는 태스크도 풀 수 있습니다. 텍스트의 쌍을 입력으로 받는 대표적인 태스크로 자연어 추론(Natural language inference)이 있습니다. 자연어 추론 문제란, 두 문장이 주어졌을 때, 하나의 문장이 다른 문장과 논리적으로 어떤 관계에 있는지를 분류하는 것입니다. 유형으로는 모순 관계(contradiction), 함의 관계(entailment), 중립 관계(neutral)가 있습니다.

텍스트의 쌍을 입력받는 이러한 태스크의 경우에는 입력 텍스트가 1개가 아니므로, 텍스트 사이에 [SEP] 토큰을 집어넣고, Sentence 0 임베딩과 Sentence 1 임베딩이라는 두 종류의 세그먼트 임베딩을 모두 사용하여 문서를 구분합니다.

### 4) 질의 응답(Question Answering)

![img](https://wikidocs.net/images/page/115055/apply4.PNG)

텍스트의 쌍을 입력으로 받는 또 다른 태스크로 QA(Question Answering)가 있습니다. BERT로 QA를 풀기 위해서 질문과 본문이라는 두 개의 텍스트의 쌍을 입력합니다. 이 태스크의 대표적인 데이터셋으로 SQuAD(Stanford Question Answering Dataset) v1.1이 있습니다. 이 데이터셋을 푸는 방법은 질문과 본문을 입력받으면, 본문의 일부분을 추출해서 질문에 답변하는 것입니다. 실제로 이 데이터셋은 영어로 되어있지만 한국어로 예시를 들어보겠습니다. "강우가 떨어지도록 영향을 주는 것은?" 라는 질문이 주어지고, "기상학에서 강우는 대기 수증기가 응결되어 중력의 영향을 받고 떨어지는 것을 의미합니다. 강우의 주요 형태는 이슬비, 비, 진눈깨비, 눈, 싸락눈 및 우박이 있습니다." 라는 본문이 주어졌다고 해보겠습니다 이 경우, 정답은 "중력"이 되어야 합니다.

## 9. 그 외 기타

- 훈련 데이터는 위키피디아(25억 단어)와 BooksCorpus(8억 단어) ≈ 33억 단어
- WordPiece 토크나이저로 토큰화를 수행 후 15% 비율에 대해서 마스크드 언어 모델 학습
- 두 문장 Sentence A와 B의 합한 길이. 즉, 최대 입력의 길이는 512로 제한
- 100만 step 훈련 ≈ (총 합 33억 단어 코퍼스에 대해 40 에포크 학습)
- 옵티마이저 : 아담(Adam)
- 학습률(learning rate) : 10−4
- 가중치 감소(Weight Decay) : L2 정규화로 0.01 적용
- 드롭 아웃 : 모든 레이어에 대해서 0.1 적용
- 활성화 함수 : relu 함수가 아닌 gelu 함수
- 배치 크기(Batch size) : 256

## 10. 어텐션 마스크(Attention Mask)

![img](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC11.PNG)

BERT를 실제로 실습하게 되면 어텐션 마스크라는 시퀀스 입력이 추가로 필요합니다. 어텐션 마스크는 BERT가 어텐션 연산을 할 때, 불필요하게 패딩 토큰에 대해서 어텐션을 하지 않도록 **실제 단어와 패딩 토큰을 구분할 수 있도록 알려주는 입력입니다.** 이 값은 0과 1 두 가지 값을 가지는데, **숫자 1**은 해당 토큰은 **실제 단어**이므로 마스킹을 하지 않는다라는 의미이고, 숫자 0은 해당 토큰은 패딩 토큰이므로 마스킹을 한다는 의미입니다. 위의 그림과 같이 실제 단어의 위치에는 1, 패딩 토큰의 위치에는 0의 값을 가지는 시퀀스를 만들어 BERT의 또 다른 입력으로 사용하면 됩니다.

다음 챕터에서 BERT를 이용하여 실제로 이진 분류, 다중 클래스 분류, 개체명 인식, QA(Question Answering)을 풀며 BERT에 대한 파인 튜닝 방법을 이해해봅시다.
