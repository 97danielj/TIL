# 17. BERT(Bidirectional Encoder Representations from Transformers)

트랜스포머(Transformer)의 등장 이후, 다영한 자연어 처리 태스크에서 사용되었던 RNN계열의 신경망인 LSTM. GRU는 트랜스포머로 대체되어가는 추세입니다.

이에 따라 다양한 트랜스포머 계열의 BERT, GPT, T5 등 다양한 사전 훈련된 언어 모델들이 계속해서 등장하고 있습니다. 여기서는 가장 유명한 사전 훈련된 언어 모델 중 하나인 BERT에 대해서 소개합니다. 이번 챕터에서 소개하는 BERT의 기본 개념을 이해하고, ALBERT, RoBERTa, ELECTRA와 같은 BERT의 파생 모델을 공부하는 방식으로 트랜스포머 계열 언어 모델에 대한 공부를 이어나가시기 바랍니다.