[toc]

# 6) RNN을 이용한 텍스트 생성(Text Generation using RNN)

<hr/>

## **1. RNN을 이용하여 텍스트 생성하기**

예를 들어서 '경마장에 있는 말이 뛰고 있다'와 '그의 말이 법이다'와 '가는 말이 고와야 오는 말이 곱다'라는 세 가지 문장이 있다고 해봅시다. **모델이 문맥을 학습**할 수 있도록 **전체 문장의 앞의 단어들을 전부 고려하여 학습하도록 데이터를 재구성**한다면 아래와 같이 총 11개의 샘플이 구성됩니다.



| samples | X                          | y      |
| :------ | :------------------------- | :----- |
| 1       | 경마장에                   | 있는   |
| 2       | 경마장에 있는              | 말이   |
| 3       | 경마장에 있는 말이         | 뛰고   |
| 4       | 경마장에 있는 말이 뛰고    | 있다   |
| 5       | 그의                       | 말이   |
| 6       | 그의 말이                  | 법이다 |
| 7       | 가는                       | 말이   |
| 8       | 가는 말이                  | 고와야 |
| 9       | 가는 말이 고와야           | 오는   |
| 10      | 가는 말이 고와야 오는      | 말이   |
| 11      | 가는 말이 고와야 오는 말이 | 곱다   |

### **1) 데이터에 대한 이해와 전처리**

```python
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
```

우선 예제로 언급한 3개의 한국어 문장을 저장합니다.

```python
text = """경마장에 있는 말이 뛰고 있다\n
그의 말이 법이다\n
가는 말이 고와야 오는 말이 곱다\n"""
```

단어 집합을 생성하고 크기를 확인해보겠습니다. 단어 집합의 크기를 저장할 때는 케라스 토크나이저의 정수 인코딩은 인덱스가 1부터 시작하지만, 패딩을 위한 0을 고려하여 +1을 해줍니다.

```go
tokenizer = Tokenizer()
tokenizer.fit_on_texts([text])
vocab_size = len(tokenizer.word_index) + 1
print('단어 집합의 크기 : %d' % vocab_size)
```

각 단어와 단어에 부여된 정수 인덱스를 출력해보겠습니다.

```scss
print(tokenizer.word_index)
{'말이': 1, '경마장에': 2, '있는': 3, '뛰고': 4, '있다': 5, '그의': 6, '법이다': 7, '가는': 8, '고와야': 9, '오는': 10, '곱다': 11}
```

훈련 데이터를 만들어보겠습니다.

```go
sequences = list()
for line in text.split('\n'): # 줄바꿈 문자를 기준으로 문장 토큰화
    encoded = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(encoded)):
        sequence = encoded[:i+1]
        sequences.append(sequence)

print('학습에 사용할 샘플의 개수: %d' % len(sequences))
학습에 사용할 샘플의 개수: 11
```

샘플의 개수는 총 11개가 나옵니다. 전체 샘플을 출력해봅시다.

```tex
print(sequences)
[[2, 3], [2, 3, 1], [2, 3, 1, 4], [2, 3, 1, 4, 5], [6, 1], [6, 1, 7], [8, 1], [8, 1, 9], [8, 1, 9, 10], [8, 1, 9, 10, 1], [8, 1, 9, 10, 1, 11]]
```

위의 데이터는 아직 레이블로 사용될 단어를 분리하지 않은 훈련 데이터입니다. [2, 3]은 [경마장에, 있는]에 해당되며 [2, 3, 1]은 [경마장에, 있는, 말이]에 해당됩니다. 전체 훈련 데이터에 대해서 맨 우측에 있는 단어에 대해서만 레이블로 분리해야 합니다.

우선 전체 샘플에 대해서 길이를 일치시켜 줍니다. 가장 긴 샘플의 길이를 기준으로 합니다. 현재 육안으로 봤을 때, 길이가 가장 긴 샘플은 [8, 1, 9, 10, 1, 11]이고 길이는 6입니다. 이를 코드로는 다음과 같이 구할 수 있습니다.

```undefined
샘플의 최대 길이 : 6
```

전체 훈련 데이터에서 가장 긴 샘플의 길이가 6임을 확인하였습니다. 전체 샘플의 길이를 6으로 패딩합니다.

```python
sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')
```

pad_sequences()는 모든 샘플에 대해서 0을 사용하여 길이를 맞춰줍니다. maxlen의 값으로 6을 주면 모든 샘플의 길이를 6으로 맞춰주며, padding의 인자로 'pre'를 주면 길이가 6보다 짧은 샘플의 앞에 0으로 채웁니다. 전체 훈련 데이터를 출력해봅니다.

```python
print(sequences)
```

```tex
[[ 0  0  0  0  2  3]
 [ 0  0  0  2  3  1]
 [ 0  0  2  3  1  4]
 [ 0  2  3  1  4  5]
 [ 0  0  0  0  6  1]
 [ 0  0  0  6  1  7]
 [ 0  0  0  0  8  1]
 [ 0  0  0  8  1  9]
 [ 0  0  8  1  9 10]
 [ 0  8  1  9 10  1]
 [ 8  1  9 10  1 11]]
```

길이가 6보다 짧은 모든 샘플에 대해서 앞에 0을 채워서 모든 샘플의 길이를 6으로 바꿨습니다. 이제 각 샘플의 마지막 단어를 레이블로 분리합시다. 레이블의 분리는 Numpy를 이용해서 가능합니다. 리스트의 마지막 값을 제외하고 저장한 것은 X, 리스트의 마지막 값만 저장한 것은 y. 이는 레이블에 해당됩니다.



| -         | 원-핫 벡터                           | 임베딩 벡터                |
| :-------- | :----------------------------------- | :------------------------- |
| 차원      | 고차원(단어 집합의 크기)             | 저차원                     |
| 다른 표현 | 대부분의 값이 0이 대부분인 희소 벡터 | 모든 값이 실수인 밀집 벡터 |
| 표현 방법 | 수동                                 | 훈련 데이터로부터 학습함   |
| 값의 타입 | 1과 0                                | 실수                       |

단어를 원-핫 벡터로 만드는 과정을 원-핫 인코딩이라고 한다면, 단어를 밀집 벡