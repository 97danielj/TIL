[TOC]

# 텍스트 전처리

## 1. 정형, 비정형, 반정형 데이터란?

1. 정형데이터(Structured data)
   1. 정형 데이터는 데이터베이스의 정해진 규칙(Rule)에 맞는 데이터
   2. 들어간 데이터 중에 수치 만으로 의미 파악이 쉬운 데이터들을 보통 말한다
   3. 그 값의 의미를 파악하기 쉽고, 규칙적인 값으로 데이터가 들어갈 경우 정형데이터라고 인식
   4. Gender, Age의 특성의 값
2. 비정형 데이터(Unstructured data)
   1. 정해진 규칙이 없어서 값의 의미를 쉽게 파악하기 힘든 경우
   2. 흔히 텍스트, 음성, 영상과 같은 데이터가 비정형 데이터 범위에 속해있다.
3. 반정형 데이터(Semi-structured data)
   1. 반정형 데이터의  반은 Semi를 의미한다. 즉 완전한 정형이 아니라 약한 정형 데이터라는 것
   2. 대표적으로 HTML이나 XML과 같은 포맷
   3. 이런 범주는 대체로 데이터베이스는 아니지만 스키마를 가지고 있는 형태이다.
   4. 데이터베이스 데이터 -> dump - > Json, XML 형태 포맷하는 순간 반정형 데이터?
      1. 구조의 차이
         1. 데이터베이스는 데이터를저장하는 장소와 스키마가 분리되어 있다.
         2. 데이터베이스는 테이블을 생성하고, 데이터를 저장
         3. 반정형 은 한 텍스트 파일에 Coulmn과 Values를 모두 출력한다.
         4. 반정형을 정형, 비정형에서 완벽 구분은 어렵다.



## 2. 텍스트 전처리

- 말뭉치(코퍼스)
  - 말뭉치 또는 코퍼스(복수 : corpora)
  - __자연언어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집합__
  - 코퍼스 분석 뿐만 아니라 언어 분석에 사용되는 실제 언어의 체계적 디지털 모음
  - 둘 이상의 코포스는 코포라
- 텍스트 전처리
  - 자연어 처리에서 크롤링 등으로 얻어낸 코포스 데이터가 전처리되지 않은 상태시 해당 데이터를 용도에 맞게 토큰화 & 정제 & 정규화를 진행 해야 함
- 테이터 분석
  - 데이터 수집(추출)
  - 데이터 전처리
    - 토큰화
    - 정제
    - 정규화



## 3. 토큰화

- 단어 토큰화(Word Tokenization)
- 문장 토큰화(Sentence Tokenization)



## 4. 단어 토큰화(Word Tokenization)

- 토큰 기준을 단어로 하여 토큰화 하는것
- 단어(word)는 단어 단위 외에도 단어구, 의미를 갖는 문자열로도 간주됨
- 보통 토큰화 작업은 단순한 구두점이나 특수문자를 전부 제거한 정제(cleaning)작업을 수행하는 것만으로 해결되지 않음
- 구두점이나 특수문자를 전부 제거하면 어떤 토큰은 의미를 잃어 버리는 경우가 발생함
- 띄어쓰기 단위로 자를시 단어 토큰 구분이 망가지는 언어도 존재
  - ex) 데이터 과학
- 토큰화 진행 시, 예상하지 못한 경우가 있어서 토큰화의 기준을 생각해봐야 하는 경우가 발생함 Ex) 영어의 어퍼스트로피 토큰화 문제 Don't

|           코드 및 모듈           |                             기능                             |
| :------------------------------: | :----------------------------------------------------------: |
|  nltk(Natural Language Toolkit)  |          자연어(영어) 처리 및 문자열 분석용 패키지           |
| konlpy(Korean Natural Language ) |             한국어 자연어 처리 및 분석용  패키지             |
|  kss(Korean Sentence Splitter)   | 한국어 문장 구분기 제공 패키지(**New Line(\n)을 포함한다.**) |

```python
from nltk.tokenize import word_tokenize
# 가장 많이 사용하는 자연어(영어) 단어 토큰화기. 형태소 기준 으로 분류

from nltk.tokenize import WordPinctTokenizer
#또 다른 단어 토큰화기 클래스를 호출. 단어 기준으로 분류

from tensorflow.keras.preprocessing.text import text_to_word_sequence
#keras의 텍스트 전처리 패키지에서 text 단어 분리기. 단어를 모두 소문자 취급

TreebankWordTokenizer().tokenize(text)
#nktl의 또 다른 단어 토큰화기 = word_tokenize()와 동일하다.

from nltk.tag import pos_tag #pos(part of sentence = 품사)
t_t = word_tokenize(t6) #단어 토큰화
pos_tag(t_t) #토큰화된 리스트의 각 원소에 품사 태그 붙인다.

```



- 토큰화 고려 사항 
  - 구두점이나 특수 문자를 단순 제외해서는 안 된다.
    - 구두점조차 하나의 토큰으로 분류하기도 한다.
    - 마침표(.)
      - 문장의 경계를 통한 단어를 추출용 기준 이용
    - 단어 자체 특수문자나 구두점
      - m.p.h Ph.D나 AT&T 특수 문자의 달러나($)나 슬래시(/)
    - 숫자 사이에 컴마(,)
      - 123,456,789
  - 줄임말과 단어 내에 띄어쓰기가 있는경우 
    - what're 는 what are의 줄임말
    - we're는 we are의 줄임말
    - New york은 단어 자체에 띄어쓰기가 존재
- Penn Treebank Tokenization의 규칙
  - 영어 토큰화 표준으로 사용되는 규칙(=word_tokenization)
  - 규칙1 : 하이푼으로 구성된 단어는 하나로 유지
  - 규칙2 : doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리



## 3. 문장 토큰화(Sentence Tokenization)

- 문장 단위로 구분하는 작업으로 때로는 문장 분류

- 정제되지 않은 상태라면, 코퍼스는 문장 단위로 구분되어 있지 않아서 이를 사용하고 하는 용도에 맞게 문장 토큰화가 필요.

- !나 ?는 문장의 구분을 위한 꽤 명확한 구분자(boundary) 역할

- 마침표. 는 문장의 끝이 아니더라도 등장할 수 있음=> 단어 자체의 . 이 들어갈 수 있다.

  - EX1) IP 192.168.56.31 서버에 들어가서 aaa@gmail.com로 굙허 좀 보냐줘
  - EX2) Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.

- ```python
  from nltk.tokenize import sent_tokenize
  #문장 토큰화기
  sent_tokenize(t3)
  ```



## 4. 한국어 토큰화

- 영어는 New York과 같은 합성어나 he's와 같이 줄임말에 대한 예외처리만 한다면, 띄어쓰기 기준으로 띄어쓰기 토큰화를 수행해도 단어토큰화가 잘 작동
- 한국어는 영어와 달리띄어 쓰기만으로는 토큰화를 하기에 부족
- 한귝어의 경우 띄어쓰기 단위가 되는 단위를 어절이라고 하는대 어절 토큰화는 한국어 NLP에서 지양되고 있음
- 어절토큰화와 단어토큰화는 다름
- 한국어가 영어랑 다른 형태를 가지는 언어인 교착어라는 점에서기인
- 교착어란 조사, 어미 등을 붙여서 말을 만드는 언어를 말함
  - 조사가 존재
    - 그라는 단어 하나에도 '그가', '그에게', '그를' ,'그와', '그는'과 같이 다양한 조사가 '그'라는 글자 뒤에 띄어쓰기 없이 바로 붙게됨
    - 대부분의 한국어 NLP에서 조사는 분리해줄 필요가 있음
  - 띄어쓰기 단이가 영어처럼 독립적인 단어가 아님
  - 한국어는 어절이 독립적인 단어로 구성되는 것이 아니라 조사 등의 무언가가 붙어있는 경우
    가 많아서 __이를 전부 분리해줘야 함__

## 5. 형태소

- 한국어 토큰화에서는 형태소란 개념을 반드시 이해해야 함
- 형태소
  - 뜻을 가진 가장 작은 말의 단위
  - 이 형태소에는 두가지 형태소가 있는데 자립과 의존
  - 한국어의 문장=> 어절(띄어쓰기 단위)로 구성 =>  어절은 다양한 형태소로 구상
- 자립 형태소
  - 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소
  - __그 자체로 단어가 됨__
  - 체언, 수식언, 감탄사
- 의존 형태소
  - 다른 형태소와 결합하여 사용되는 형태소
  - 접사, 조사, 어미, 어간
- 한국어에서 영어에서의 단어 토큰화와 유사한 형태를 얻어내려면 어절토큰화가 아난 형태소 토큰화를 수행해야 한다.

```python
from konlpy.tag import Okt #세종 품사 사전. 오픈 코리언 태그 형태소
from konlpy.tag import Kkma #서울대 제작 사전

nl = Okt() #형태소 토큰화기 객체
n1.morphs(text) #텍스트에서 형태소 반환한다.
n1.pos(text) #텍스트에서 품사 정보를 부착하여 반환한다.
#영어 단어 토큰화에서는 pos_tag(토큰화된 리스트)

n1.nouns(text)

n2=Kkma() #어절,조사,접사 모두 형태소로 분리
n2.morphs(t7),n2.pos(t7),n2.nouns(t7)

```

- 품사 태깅(part-of-speech tagging) – 단어 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지를 구분해놓는 작업



## 6. 정제(cleaning)

- 갖고 있는 코퍼스로부터 노이즈 데이터를 제거
- 완벽한 정제작업은 어렵다.
- 일종의 합의점을 찾음



## 7. 정규화(nomalization)

- 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들기
- 규칙에 기반한 통합
  - 정규화 규칙의 예로서 같은 의미를 갖고있음에도, 표기가 다른 단어들을 하나의 단어로 정규화하는 방법을 사용
- 대,소문자 통합
  - 대부분의 글은 소문자로 작성되기 때문에 대.소문자 통합 작업은 대부분 대문자를 소문자로 변환하는 소문자 변환작업
- 불필요한 단어의 제거
  - 등장빈도가 적은 단어
  - 길이가 짧은 단어
- 단어의 개수 줄일 수 있는 기법
  - 표제어 추출
  - 어간 추추출
- 하나의 단어로 일반화시킬  수 있다면 하나의 단어로 일반화 시켜서 문서 내의 단어 수를 줄이겠다는 것
