[toc]

# 08) 사전 훈련된 워드 임베딩(Pre-trained Word Embedding)

이번에는 **케라스의 임베딩 층(embedding layer)** 과 **사전 훈련된 워드 임베딩(pre-trained word embedding)** 을 가져와서 사용하는 것을 비교해봅니다. 자연어 처리를 하려고 할 때 갖고 있는 **훈련 데이터의 단어들을 임베딩 층(embedding layer)을 구현하여 임베딩 벡터로 학습**하는 경우가 있습니다. 케라스에서는 이를 **Embedding()**이라는 도구를 사용하여 구현합니다.

위키피디아 등과 같은 방대한 코퍼스를 가지고 Word2vec, FastText, GloVe 등을 통해서 미리 훈련된 임베딩 벡터를 불러오는 방법을 사용하는 경우도 있습니다. 이는 현재 갖고 있는 훈련 데이터를 임베딩 층으로 처음부터 학습을 하는 방법과는 대조됩니다.

## **1. 케라스 임베딩 층(Keras Embedding layer)**

케라스는 훈련 데이터의 단어들에 대해 워드 임베딩을 수행하는 도구 Embedding()을 제공합니다. Embedding()은 인공 신경망 구조 관점에서 임베딩 층(embedding layer)을 구현합니다.

### **1) 임베딩 층은 룩업 테이블이다.**

임베딩 층의 입력으로 사용하기 위해서 입력 시퀀스의 각 단어들은 모두 정수 인코딩이 되어있어야 합니다.

어떤 단어 → 단어에 부여된 고유한 정수값 → 임베딩 층 통과 → 밀집 벡터

```python
#ㅇ ㅈㅇ
impo
```



